{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "source": [
    "#Created 2021-09-08\n",
    "#Copyright Spencer W. Leifeld"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "source": [
    "from pyspark.sql import SparkSession as Session\n",
    "from pyspark import SparkConf as Conf\n",
    "from pyspark import SparkContext as Context"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "source": [
    "import os\n",
    "os.environ['SPARK_LOCAL_IP']='192.168.1.2'\n",
    "os.environ['HADOOP_HOME']='/home/geno1664/Developments/Github_Samples/RDS-ENV/hadoop'\n",
    "os.environ['LD_LIBRARY_PATH']='$LD_LIBRARY_PATH:$HADOOP_HOME/lib/native'\n",
    "os.environ['PYSPARK_DRIVER_PYTHON']='jupyter'\n",
    "os.environ['PYSPARK_DRIVER_PYTHON_OPTS']='notebook'\n",
    "os.environ['PYSPARK_PYTHON']='python3'\n",
    "os.environ['PYARROW_IGNORE_TIMEZONE']='1'"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "source": [
    "configuration = Conf().setAppName('RDS_2').setMaster('spark://GenoMachine:7077')\n",
    "configuration.set('spark.executor.memory','10G').set('spark.driver.memory', '2G').set('spark.cores.max', '8')"
   ],
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "<pyspark.conf.SparkConf at 0x7fd619252850>"
      ]
     },
     "metadata": {},
     "execution_count": 4
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "source": [
    "context = Context(conf=configuration)"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "21/09/09 11:18:35 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
      "Using Spark's default log4j profile: org/apache/spark/log4j-defaults.properties\n",
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "source": [
    "session = Session(context)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "source": [
    "from Functions.IO import CSV_File"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "source": [
    "csvDF = CSV_File(session, r'/home/geno1664/Developments/Github_Samples/RDS-ENV/Rural_Development_Study_No2/IO/Jobs.csv')\n",
    "employmentByJob = csvDF.GetSparkDF().select('State', 'County', 'PctEmpAgriculture', 'PctEmpConstruction', 'PctEmpMining', 'PctEmpTrade', 'PctEmpTrans', \\\n",
    "    'PctEmpInformation', 'PctEmpFIRE', 'PctEmpServices', 'PctEmpGovt', 'PctEmpManufacturing')"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Now Reading: /home/geno1664/Developments/Github_Samples/RDS-ENV/Rural_Development_Study_No2/IO/Jobs.csv\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stderr",
     "text": []
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "source": [
    "employmentByJob = employmentByJob.withColumnRenamed('PctEmpAgriculture', 'Farmers').withColumnRenamed('PctEmpConstruction', 'Builders').withColumnRenamed('PctEmpMining', 'Miners') \\\n",
    "    .withColumnRenamed('PctEmpTrade', 'Retail_Associates').withColumnRenamed('PctEmpFIRE', 'Businessmen').withColumnRenamed('PctEmpServices', 'Hospitality_Associates') \\\n",
    "        .withColumnRenamed('PctEmpGovt', 'Civil_Servants').withColumnRenamed('PctEmpManufacturing', 'Craftsmen').withColumnRenamed('PctEmpInformation', 'Technologists') \\\n",
    "            .withColumnRenamed('PctEmpTrans', 'Teamsters')"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "source": [
    "employmentByJob = employmentByJob.where(employmentByJob.State != 'US')\n",
    "employmentByJob = employmentByJob.repartition('State')"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "source": [
    "employmentByJob.show()"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "+-----+--------------+-------------+------------+-------------+-----------------+-------------+-------------+-------------+----------------------+--------------+-------------+\n",
      "|State|        County|      Farmers|    Builders|       Miners|Retail_Associates|    Teamsters|Technologists|  Businessmen|Hospitality_Associates|Civil_Servants|    Craftsmen|\n",
      "+-----+--------------+-------------+------------+-------------+-----------------+-------------+-------------+-------------+----------------------+--------------+-------------+\n",
      "|   AZ|       Arizona| 0.9066464622|6.9362734607|  0.461149062|    14.3824716721| 5.3067438219| 1.8067447802| 8.4040479669|         49.9525658823|  4.8267488815| 7.0166080102|\n",
      "|   AZ|        Apache| 1.7880434783|9.0217391304| 1.0815217391|     9.6847826087| 7.6032608696| 0.6684782609| 3.1847826087|         53.5054347826| 11.4456521739| 2.0163043478|\n",
      "|   AZ|       Cochise|  2.239488117|6.7664533821| 0.4981718464|    12.9547531993| 4.4881170018| 1.4008226691| 4.1933272395|         47.4428702011| 16.5036563071| 3.5123400366|\n",
      "|   AZ|      Coconino|  1.132894343|5.1780903921| 0.2065249925|    11.5698892547| 4.7381023646| 1.0116731518| 3.6815324753|         60.2349595929|  5.8380724334| 6.4082609997|\n",
      "|   AZ|          Gila| 2.0326290452|8.9382187751| 5.8250869216|    11.9604172239| 4.4878309708| 1.3426049746| 4.6589997325|         48.5958812517|  7.9219042525| 4.2364268521|\n",
      "|   AZ|        Graham|  2.938034188|7.5167887668| 9.6535409035|    16.5216727717|  3.083028083| 1.4117826618| 3.4493284493|         43.5821123321|  7.5015262515| 4.3421855922|\n",
      "|   AZ|      Greenlee| 2.1447028424|7.4160206718|38.7080103359|     9.9483204134| 2.7906976744| 0.2583979328| 1.3953488372|         31.6795865633|  3.9018087855| 1.7571059432|\n",
      "|   AZ|        La Paz|10.5194805195|4.6915584416| 0.6980519481|     17.224025974| 5.7467532468| 1.1525974026|  2.224025974|         42.0454545455| 10.5519480519| 5.1461038961|\n",
      "|   AZ|      Maricopa| 0.4348187667| 7.077300362|  0.155689584|    14.5641803717| 5.5263516709|  1.993551937|10.0048447504|         49.1126660729|  3.6962737971| 7.4343226875|\n",
      "|   AZ|        Mohave| 0.5470849558|6.9475675972| 0.3277025174|    16.1451763286| 5.8492842648| 1.6056052213| 5.3447046564|          52.003235891|  5.2336422969| 5.9959962705|\n",
      "|   AZ|        Navajo| 1.7267694149|8.0472841538| 1.4185246147|    14.5563369744| 6.4132874458| 1.3377225797| 4.2675445159|         50.2768217866|   9.097710609| 2.8579979051|\n",
      "|   AZ|          Pima| 0.4919542187|6.5273543123| 0.6106863389|    13.4167295798| 4.3778548914| 1.6333507324| 5.7638843773|         54.5515846258|  5.9341417572| 6.6924591662|\n",
      "|   AZ|         Pinal| 1.9860834738|6.2700414454| 1.0133208279|    14.8037364225| 5.2478485152| 1.3758095793| 6.8860188342|         46.5265719464|  7.3061762506| 8.5843927046|\n",
      "|   AZ|    Santa Cruz| 0.7166579269|6.1935559051| 0.8739730816|    21.7444502709|11.4315679077| 0.8331876712| 4.4980481268|         40.7329720911|   8.180388044| 4.7951989745|\n",
      "|   AZ|       Yavapai| 0.8167020684|7.9273487284| 1.5788816066|    15.2117859536| 4.4356350171| 1.5936481252| 4.6900734918|         53.3593829867|  4.7764008315| 5.6101411906|\n",
      "|   AZ|          Yuma|11.4369656008|6.1064780048| 0.2490020157|    14.4987681646| 4.6190532654| 1.0236749536| 3.9063014637|         43.4752249582| 10.1234470311| 4.5610845421|\n",
      "|   SC|South Carolina| 0.8771579029| 6.824077545| 0.0843759105|    14.3167023433| 5.0981507173| 1.6106570291| 5.7970205636|         47.3103200967|  4.4240662949|13.6574715967|\n",
      "|   SC|     Abbeville| 1.7121931908|8.9172604909|          NaN|    10.4612034838| 3.2462391132| 1.7715756136| 2.7019002375|         44.2992874109|  3.1967537609|23.6935866983|\n",
      "|   SC|         Aiken|  1.067635629|7.4636288388| 0.3002286789|     13.552378681| 6.3917843965| 1.0704415045| 4.5230713113|         46.3586750656|  5.1501844863|14.1219714081|\n",
      "|   SC|     Allendale| 7.0588235294|3.7370242215|          NaN|     7.3702422145| 7.7162629758| 0.7266435986| 2.3529411765|         42.0069204152| 15.2595155709|13.7716262976|\n",
      "+-----+--------------+-------------+------------+-------------+-----------------+-------------+-------------+-------------+----------------------+--------------+-------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "source": [
    "csvDF = CSV_File(session, r'/home/geno1664/Developments/Github_Samples/RDS-ENV/Rural_Development_Study_No2/IO/People.csv')\n",
    "educationRate = csvDF.GetSparkDF().select('State', 'County', 'Ed1LessThanHSPct', 'Ed2HSDiplomaOnlyPct', 'Ed3SomeCollegePct', 'Ed4AssocDegreePct', 'Ed5CollegePlusPct')"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Now Reading: /home/geno1664/Developments/Github_Samples/RDS-ENV/Rural_Development_Study_No2/IO/People.csv\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "source": [
    "educationRate = educationRate.withColumnRenamed('Ed1LessThanHSPct', 'Some_High_School').withColumnRenamed('Ed2HSDiplomaOnlyPct', 'High_School_Degree') \\\n",
    "    .withColumnRenamed('Ed3SomeCollegePct', 'Some_College').withColumnRenamed('Ed4AssocDegreePct', 'Associates_Degree').withColumnRenamed('Ed5CollegePlusPct', 'College_Graduate')"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "source": [
    "educationRate = educationRate.where(educationRate.State != 'US')\n",
    "educationRate = educationRate.repartition('State')"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "source": [
    "educationRate.show()"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "+-----+--------------+----------------+------------------+-------------+-----------------+----------------+\n",
      "|State|        County|Some_High_School|High_School_Degree| Some_College|Associates_Degree|College_Graduate|\n",
      "+-----+--------------+----------------+------------------+-------------+-----------------+----------------+\n",
      "|   AZ|       Arizona|   12.8607054321|     23.8588772353|25.1858413213|     8.6277705042|   29.4668055071|\n",
      "|   AZ|        Apache|   19.5977663574|      32.238759483| 28.234221006|     7.6286457986|    12.300607355|\n",
      "|   AZ|       Cochise|   12.6165291708|     24.1034242746|28.7691552847|    11.4013167654|   23.1095745045|\n",
      "|   AZ|      Coconino|    9.4800192585|     22.0389985556|23.8854116514|     8.4352431391|   36.1603273953|\n",
      "|   AZ|          Gila|   13.6740401729|     27.8540554284|30.8619374523|     8.8761759471|   18.7337909992|\n",
      "|   AZ|        Graham|   14.7431030119|     28.7226862398|30.8529486206|    10.5289800051|   15.1522821227|\n",
      "|   AZ|      Greenlee|   11.9405534677|      34.711308507|30.7311240178|     9.1048855483|   13.5121284592|\n",
      "|   AZ|        La Paz|   23.0611100729|     31.1468261384|27.3033077929|     6.4785398368|    12.010216159|\n",
      "|   AZ|      Maricopa|   12.2741475225|     22.3936237653| 24.116624375|     8.5175323875|   32.6980719497|\n",
      "|   AZ|        Mohave|   13.9326098074|     35.3441799092|29.4511683113|      8.326230657|    12.945811315|\n",
      "|   AZ|        Navajo|   16.6182204892|     30.3197447714|  29.94966324|     7.8582063098|   15.2541651896|\n",
      "|   AZ|          Pima|   11.6155183744|     22.2479078342|25.0945036667|     8.6588621177|   32.3832080069|\n",
      "|   AZ|         Pinal|    14.233029233|     29.2657832058|27.6249110582|     9.3441522216|   19.5321242814|\n",
      "|   AZ|    Santa Cruz|   23.4011330285|     28.6601597161|21.3432530203|     6.1634018156|   20.4320524196|\n",
      "|   AZ|       Yavapai|    8.9507176897|     25.8408241304|29.7434580373|     9.5876494592|   25.8773506834|\n",
      "|   AZ|          Yuma|   26.7469588578|     25.7078390534|24.7796492499|     7.7394427843|   15.0261100546|\n",
      "|   SC|South Carolina|   12.4889649492|     29.1031459214|20.5041179633|     9.7840393783|   28.1197317877|\n",
      "|   SC|     Abbeville|    18.349106204|     34.0285080033|18.9332865989|    13.1090080617|   15.5800911321|\n",
      "|   SC|         Aiken|   12.0274768556|     32.7587813286|20.3188128372|     8.4776771724|   26.4172518062|\n",
      "|   SC|     Allendale|   20.2017018594|     45.8556571068|18.0586196029|     6.6656161361|    9.2184052947|\n",
      "+-----+--------------+----------------+------------------+-------------+-----------------+----------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "source": [
    "from databricks import koalas as ks"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "source": [
    "employmentByJob = employmentByJob.to_koalas().melt(id_vars=['State', 'County'], var_name='Employment_Catagory', value_name='Employment_Rate').to_spark()\n",
    "educationRate = educationRate.to_koalas().melt(id_vars=['State', 'County'], var_name='Education_Catagory', value_name='Education_Rate').to_spark()"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "source": [
    "mainDF = employmentByJob.join(educationRate, on=['State', 'County'], how='cross') \\\n",
    "    .fillna(0, subset=['Employment_Rate', 'Education_Rate']).fillna('NONE', subset=['Employment_Catagory', 'Education_Catagory'])"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "source": [
    "mainDF.show()"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "text": []
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "+-----+-------+-------------------+---------------+------------------+--------------+\n",
      "|State| County|Employment_Catagory|Employment_Rate|Education_Catagory|Education_Rate|\n",
      "+-----+-------+-------------------+---------------+------------------+--------------+\n",
      "|   AZ|Arizona|            Farmers|   0.9066464622|  College_Graduate| 29.4668055071|\n",
      "|   AZ|Arizona|            Farmers|   0.9066464622| Associates_Degree|  8.6277705042|\n",
      "|   AZ|Arizona|            Farmers|   0.9066464622|      Some_College| 25.1858413213|\n",
      "|   AZ|Arizona|            Farmers|   0.9066464622|High_School_Degree| 23.8588772353|\n",
      "|   AZ|Arizona|            Farmers|   0.9066464622|  Some_High_School| 12.8607054321|\n",
      "|   AZ|Arizona|           Builders|   6.9362734607|  College_Graduate| 29.4668055071|\n",
      "|   AZ|Arizona|           Builders|   6.9362734607| Associates_Degree|  8.6277705042|\n",
      "|   AZ|Arizona|           Builders|   6.9362734607|      Some_College| 25.1858413213|\n",
      "|   AZ|Arizona|           Builders|   6.9362734607|High_School_Degree| 23.8588772353|\n",
      "|   AZ|Arizona|           Builders|   6.9362734607|  Some_High_School| 12.8607054321|\n",
      "|   AZ|Arizona|             Miners|    0.461149062|  College_Graduate| 29.4668055071|\n",
      "|   AZ|Arizona|             Miners|    0.461149062| Associates_Degree|  8.6277705042|\n",
      "|   AZ|Arizona|             Miners|    0.461149062|      Some_College| 25.1858413213|\n",
      "|   AZ|Arizona|             Miners|    0.461149062|High_School_Degree| 23.8588772353|\n",
      "|   AZ|Arizona|             Miners|    0.461149062|  Some_High_School| 12.8607054321|\n",
      "|   AZ|Arizona|  Retail_Associates|  14.3824716721|  College_Graduate| 29.4668055071|\n",
      "|   AZ|Arizona|  Retail_Associates|  14.3824716721| Associates_Degree|  8.6277705042|\n",
      "|   AZ|Arizona|  Retail_Associates|  14.3824716721|      Some_College| 25.1858413213|\n",
      "|   AZ|Arizona|  Retail_Associates|  14.3824716721|High_School_Degree| 23.8588772353|\n",
      "|   AZ|Arizona|  Retail_Associates|  14.3824716721|  Some_High_School| 12.8607054321|\n",
      "+-----+-------+-------------------+---------------+------------------+--------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "source": [
    "mainDF = mainDF.to_koalas()\n",
    "mainDF['Hueristic'] = (mainDF['Employment_Rate'] / 100) * (mainDF['Education_Rate'] / 100) * 100\n",
    "mainDF = mainDF.to_spark().select('Employment_Catagory', 'Education_Catagory', 'Hueristic')"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "source": [
    "mainDF = mainDF.groupBy('Employment_Catagory').pivot('Education_Catagory').mean()"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "text": []
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "source": [
    "mainDF.show()"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "text": []
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "+--------------------+-------------------+------------------+------------------+-------------------+-------------------+\n",
      "| Employment_Catagory|  Associates_Degree|  College_Graduate|High_School_Degree|       Some_College|   Some_High_School|\n",
      "+--------------------+-------------------+------------------+------------------+-------------------+-------------------+\n",
      "|      Civil_Servants| 0.4991104160220651|1.2303817172617337|1.9110977940676062| 1.2117440410274674| 0.7875654587994716|\n",
      "|             Farmers|0.47202349285225276|0.9679238794088135|1.6797872795044708|  1.095120845562527| 0.6669638029817897|\n",
      "|           Teamsters| 0.5024450736540681|1.1749227199195111|1.9199246920936035|  1.202032973463126| 0.7515646439894451|\n",
      "|Hospitality_Assoc...|  3.935964409264519|10.041532500582806|14.451049048084114|  9.269000425159337|  5.665075456559447|\n",
      "|           Craftsmen| 1.1229021642002426|2.4930672079499114| 4.326239662101073|  2.558292794172981| 1.6696027899509687|\n",
      "|            Builders| 0.6699352044400888|1.6028506972195458|2.5252228004261936| 1.5947577638466666| 0.9909776943898683|\n",
      "|   Retail_Associates| 1.2503402365356289| 3.007014284284859| 4.627269661442165|   2.92366568811121| 1.8138084026862697|\n",
      "|         Businessmen|0.41945193672782904|1.1147968785080435|1.4900064972792575| 0.9758996792495697| 0.5636743692177182|\n",
      "|              Miners|0.12237693936802581|0.2592785843139131|0.5196688663459296| 0.3294158221097878|0.22655149660944368|\n",
      "|       Technologists|0.12194626782785829| 0.330375580821019| 0.434600252554067|0.28573413322552266|0.16474194351938426|\n",
      "+--------------------+-------------------+------------------+------------------+-------------------+-------------------+\n",
      "\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "source": [
    "CSV_File.ExportCSV('/home/geno1664/Developments/Github_Samples/RDS-ENV/Rural_Development_Study_No2/IO/output.csv', mainDF)"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "21/09/09 11:18:56 WARN TaskSetManager: Lost task 59.0 in stage 51.0 (TID 2234) (192.168.1.2 executor 0): java.io.IOException: Mkdirs failed to create file:/home/geno1664/Developments/Github_Samples/RDS-ENV/Rural_Development_Study_No2/IO/output.csv/_temporary/0/_temporary/attempt_202109091118551324996356683812108_0051_m_000059_2234 (exists=false, cwd=file:/home/hadoop/.workspace/app-20210909111836-0003/0)\n",
      "\tat org.apache.hadoop.fs.ChecksumFileSystem.create(ChecksumFileSystem.java:458)\n",
      "\tat org.apache.hadoop.fs.ChecksumFileSystem.create(ChecksumFileSystem.java:443)\n",
      "\tat org.apache.hadoop.fs.FileSystem.create(FileSystem.java:1118)\n",
      "\tat org.apache.hadoop.fs.FileSystem.create(FileSystem.java:1098)\n",
      "\tat org.apache.hadoop.fs.FileSystem.create(FileSystem.java:987)\n",
      "\tat org.apache.spark.sql.execution.datasources.CodecStreams$.createOutputStream(CodecStreams.scala:81)\n",
      "\tat org.apache.spark.sql.execution.datasources.CodecStreams$.createOutputStreamWriter(CodecStreams.scala:92)\n",
      "\tat org.apache.spark.sql.execution.datasources.csv.CsvOutputWriter.<init>(CsvOutputWriter.scala:38)\n",
      "\tat org.apache.spark.sql.execution.datasources.csv.CSVFileFormat$$anon$1.newInstance(CSVFileFormat.scala:84)\n",
      "\tat org.apache.spark.sql.execution.datasources.SingleDirectoryDataWriter.newOutputWriter(FileFormatDataWriter.scala:126)\n",
      "\tat org.apache.spark.sql.execution.datasources.SingleDirectoryDataWriter.<init>(FileFormatDataWriter.scala:111)\n",
      "\tat org.apache.spark.sql.execution.datasources.FileFormatWriter$.executeTask(FileFormatWriter.scala:269)\n",
      "\tat org.apache.spark.sql.execution.datasources.FileFormatWriter$.$anonfun$write$15(FileFormatWriter.scala:210)\n",
      "\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:90)\n",
      "\tat org.apache.spark.scheduler.Task.run(Task.scala:131)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$3(Executor.scala:497)\n",
      "\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1439)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:500)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n",
      "\tat java.lang.Thread.run(Thread.java:748)\n",
      "\n",
      "21/09/09 11:18:56 WARN TaskSetManager: Lost task 11.0 in stage 51.0 (TID 2231) (192.168.1.2 executor 0): java.io.IOException: Mkdirs failed to create file:/home/geno1664/Developments/Github_Samples/RDS-ENV/Rural_Development_Study_No2/IO/output.csv/_temporary/0/_temporary/attempt_202109091118553547660034659957556_0051_m_000011_2231 (exists=false, cwd=file:/home/hadoop/.workspace/app-20210909111836-0003/0)\n",
      "\tat org.apache.hadoop.fs.ChecksumFileSystem.create(ChecksumFileSystem.java:458)\n",
      "\tat org.apache.hadoop.fs.ChecksumFileSystem.create(ChecksumFileSystem.java:443)\n",
      "\tat org.apache.hadoop.fs.FileSystem.create(FileSystem.java:1118)\n",
      "\tat org.apache.hadoop.fs.FileSystem.create(FileSystem.java:1098)\n",
      "\tat org.apache.hadoop.fs.FileSystem.create(FileSystem.java:987)\n",
      "\tat org.apache.spark.sql.execution.datasources.CodecStreams$.createOutputStream(CodecStreams.scala:81)\n",
      "\tat org.apache.spark.sql.execution.datasources.CodecStreams$.createOutputStreamWriter(CodecStreams.scala:92)\n",
      "\tat org.apache.spark.sql.execution.datasources.csv.CsvOutputWriter.<init>(CsvOutputWriter.scala:38)\n",
      "\tat org.apache.spark.sql.execution.datasources.csv.CSVFileFormat$$anon$1.newInstance(CSVFileFormat.scala:84)\n",
      "\tat org.apache.spark.sql.execution.datasources.SingleDirectoryDataWriter.newOutputWriter(FileFormatDataWriter.scala:126)\n",
      "\tat org.apache.spark.sql.execution.datasources.SingleDirectoryDataWriter.<init>(FileFormatDataWriter.scala:111)\n",
      "\tat org.apache.spark.sql.execution.datasources.FileFormatWriter$.executeTask(FileFormatWriter.scala:269)\n",
      "\tat org.apache.spark.sql.execution.datasources.FileFormatWriter$.$anonfun$write$15(FileFormatWriter.scala:210)\n",
      "\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:90)\n",
      "\tat org.apache.spark.scheduler.Task.run(Task.scala:131)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$3(Executor.scala:497)\n",
      "\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1439)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:500)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n",
      "\tat java.lang.Thread.run(Thread.java:748)\n",
      "\n",
      "21/09/09 11:18:56 WARN TaskSetManager: Lost task 103.0 in stage 51.0 (TID 2236) (192.168.1.2 executor 0): java.io.IOException: Mkdirs failed to create file:/home/geno1664/Developments/Github_Samples/RDS-ENV/Rural_Development_Study_No2/IO/output.csv/_temporary/0/_temporary/attempt_202109091118554893928808541249322_0051_m_000103_2236 (exists=false, cwd=file:/home/hadoop/.workspace/app-20210909111836-0003/0)\n",
      "\tat org.apache.hadoop.fs.ChecksumFileSystem.create(ChecksumFileSystem.java:458)\n",
      "\tat org.apache.hadoop.fs.ChecksumFileSystem.create(ChecksumFileSystem.java:443)\n",
      "\tat org.apache.hadoop.fs.FileSystem.create(FileSystem.java:1118)\n",
      "\tat org.apache.hadoop.fs.FileSystem.create(FileSystem.java:1098)\n",
      "\tat org.apache.hadoop.fs.FileSystem.create(FileSystem.java:987)\n",
      "\tat org.apache.spark.sql.execution.datasources.CodecStreams$.createOutputStream(CodecStreams.scala:81)\n",
      "\tat org.apache.spark.sql.execution.datasources.CodecStreams$.createOutputStreamWriter(CodecStreams.scala:92)\n",
      "\tat org.apache.spark.sql.execution.datasources.csv.CsvOutputWriter.<init>(CsvOutputWriter.scala:38)\n",
      "\tat org.apache.spark.sql.execution.datasources.csv.CSVFileFormat$$anon$1.newInstance(CSVFileFormat.scala:84)\n",
      "\tat org.apache.spark.sql.execution.datasources.SingleDirectoryDataWriter.newOutputWriter(FileFormatDataWriter.scala:126)\n",
      "\tat org.apache.spark.sql.execution.datasources.SingleDirectoryDataWriter.<init>(FileFormatDataWriter.scala:111)\n",
      "\tat org.apache.spark.sql.execution.datasources.FileFormatWriter$.executeTask(FileFormatWriter.scala:269)\n",
      "\tat org.apache.spark.sql.execution.datasources.FileFormatWriter$.$anonfun$write$15(FileFormatWriter.scala:210)\n",
      "\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:90)\n",
      "\tat org.apache.spark.scheduler.Task.run(Task.scala:131)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$3(Executor.scala:497)\n",
      "\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1439)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:500)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n",
      "\tat java.lang.Thread.run(Thread.java:748)\n",
      "\n",
      "21/09/09 11:18:56 WARN TaskSetManager: Lost task 8.0 in stage 51.0 (TID 2230) (192.168.1.2 executor 0): java.io.IOException: Mkdirs failed to create file:/home/geno1664/Developments/Github_Samples/RDS-ENV/Rural_Development_Study_No2/IO/output.csv/_temporary/0/_temporary/attempt_202109091118558032137482916077310_0051_m_000008_2230 (exists=false, cwd=file:/home/hadoop/.workspace/app-20210909111836-0003/0)\n",
      "\tat org.apache.hadoop.fs.ChecksumFileSystem.create(ChecksumFileSystem.java:458)\n",
      "\tat org.apache.hadoop.fs.ChecksumFileSystem.create(ChecksumFileSystem.java:443)\n",
      "\tat org.apache.hadoop.fs.FileSystem.create(FileSystem.java:1118)\n",
      "\tat org.apache.hadoop.fs.FileSystem.create(FileSystem.java:1098)\n",
      "\tat org.apache.hadoop.fs.FileSystem.create(FileSystem.java:987)\n",
      "\tat org.apache.spark.sql.execution.datasources.CodecStreams$.createOutputStream(CodecStreams.scala:81)\n",
      "\tat org.apache.spark.sql.execution.datasources.CodecStreams$.createOutputStreamWriter(CodecStreams.scala:92)\n",
      "\tat org.apache.spark.sql.execution.datasources.csv.CsvOutputWriter.<init>(CsvOutputWriter.scala:38)\n",
      "\tat org.apache.spark.sql.execution.datasources.csv.CSVFileFormat$$anon$1.newInstance(CSVFileFormat.scala:84)\n",
      "\tat org.apache.spark.sql.execution.datasources.SingleDirectoryDataWriter.newOutputWriter(FileFormatDataWriter.scala:126)\n",
      "\tat org.apache.spark.sql.execution.datasources.SingleDirectoryDataWriter.<init>(FileFormatDataWriter.scala:111)\n",
      "\tat org.apache.spark.sql.execution.datasources.FileFormatWriter$.executeTask(FileFormatWriter.scala:269)\n",
      "\tat org.apache.spark.sql.execution.datasources.FileFormatWriter$.$anonfun$write$15(FileFormatWriter.scala:210)\n",
      "\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:90)\n",
      "\tat org.apache.spark.scheduler.Task.run(Task.scala:131)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$3(Executor.scala:497)\n",
      "\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1439)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:500)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n",
      "\tat java.lang.Thread.run(Thread.java:748)\n",
      "\n",
      "21/09/09 11:18:56 WARN TaskSetManager: Lost task 79.0 in stage 51.0 (TID 2235) (192.168.1.2 executor 0): java.io.IOException: Mkdirs failed to create file:/home/geno1664/Developments/Github_Samples/RDS-ENV/Rural_Development_Study_No2/IO/output.csv/_temporary/0/_temporary/attempt_202109091118554987526223084106802_0051_m_000079_2235 (exists=false, cwd=file:/home/hadoop/.workspace/app-20210909111836-0003/0)\n",
      "\tat org.apache.hadoop.fs.ChecksumFileSystem.create(ChecksumFileSystem.java:458)\n",
      "\tat org.apache.hadoop.fs.ChecksumFileSystem.create(ChecksumFileSystem.java:443)\n",
      "\tat org.apache.hadoop.fs.FileSystem.create(FileSystem.java:1118)\n",
      "\tat org.apache.hadoop.fs.FileSystem.create(FileSystem.java:1098)\n",
      "\tat org.apache.hadoop.fs.FileSystem.create(FileSystem.java:987)\n",
      "\tat org.apache.spark.sql.execution.datasources.CodecStreams$.createOutputStream(CodecStreams.scala:81)\n",
      "\tat org.apache.spark.sql.execution.datasources.CodecStreams$.createOutputStreamWriter(CodecStreams.scala:92)\n",
      "\tat org.apache.spark.sql.execution.datasources.csv.CsvOutputWriter.<init>(CsvOutputWriter.scala:38)\n",
      "\tat org.apache.spark.sql.execution.datasources.csv.CSVFileFormat$$anon$1.newInstance(CSVFileFormat.scala:84)\n",
      "\tat org.apache.spark.sql.execution.datasources.SingleDirectoryDataWriter.newOutputWriter(FileFormatDataWriter.scala:126)\n",
      "\tat org.apache.spark.sql.execution.datasources.SingleDirectoryDataWriter.<init>(FileFormatDataWriter.scala:111)\n",
      "\tat org.apache.spark.sql.execution.datasources.FileFormatWriter$.executeTask(FileFormatWriter.scala:269)\n",
      "\tat org.apache.spark.sql.execution.datasources.FileFormatWriter$.$anonfun$write$15(FileFormatWriter.scala:210)\n",
      "\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:90)\n",
      "\tat org.apache.spark.scheduler.Task.run(Task.scala:131)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$3(Executor.scala:497)\n",
      "\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1439)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:500)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n",
      "\tat java.lang.Thread.run(Thread.java:748)\n",
      "\n",
      "21/09/09 11:18:56 WARN TaskSetManager: Lost task 130.0 in stage 51.0 (TID 2237) (192.168.1.2 executor 0): java.io.IOException: Mkdirs failed to create file:/home/geno1664/Developments/Github_Samples/RDS-ENV/Rural_Development_Study_No2/IO/output.csv/_temporary/0/_temporary/attempt_202109091118556915927012060715031_0051_m_000130_2237 (exists=false, cwd=file:/home/hadoop/.workspace/app-20210909111836-0003/0)\n",
      "\tat org.apache.hadoop.fs.ChecksumFileSystem.create(ChecksumFileSystem.java:458)\n",
      "\tat org.apache.hadoop.fs.ChecksumFileSystem.create(ChecksumFileSystem.java:443)\n",
      "\tat org.apache.hadoop.fs.FileSystem.create(FileSystem.java:1118)\n",
      "\tat org.apache.hadoop.fs.FileSystem.create(FileSystem.java:1098)\n",
      "\tat org.apache.hadoop.fs.FileSystem.create(FileSystem.java:987)\n",
      "\tat org.apache.spark.sql.execution.datasources.CodecStreams$.createOutputStream(CodecStreams.scala:81)\n",
      "\tat org.apache.spark.sql.execution.datasources.CodecStreams$.createOutputStreamWriter(CodecStreams.scala:92)\n",
      "\tat org.apache.spark.sql.execution.datasources.csv.CsvOutputWriter.<init>(CsvOutputWriter.scala:38)\n",
      "\tat org.apache.spark.sql.execution.datasources.csv.CSVFileFormat$$anon$1.newInstance(CSVFileFormat.scala:84)\n",
      "\tat org.apache.spark.sql.execution.datasources.SingleDirectoryDataWriter.newOutputWriter(FileFormatDataWriter.scala:126)\n",
      "\tat org.apache.spark.sql.execution.datasources.SingleDirectoryDataWriter.<init>(FileFormatDataWriter.scala:111)\n",
      "\tat org.apache.spark.sql.execution.datasources.FileFormatWriter$.executeTask(FileFormatWriter.scala:269)\n",
      "\tat org.apache.spark.sql.execution.datasources.FileFormatWriter$.$anonfun$write$15(FileFormatWriter.scala:210)\n",
      "\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:90)\n",
      "\tat org.apache.spark.scheduler.Task.run(Task.scala:131)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$3(Executor.scala:497)\n",
      "\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1439)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:500)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n",
      "\tat java.lang.Thread.run(Thread.java:748)\n",
      "\n",
      "21/09/09 11:18:56 WARN TaskSetManager: Lost task 33.0 in stage 51.0 (TID 2233) (192.168.1.2 executor 0): java.io.IOException: Mkdirs failed to create file:/home/geno1664/Developments/Github_Samples/RDS-ENV/Rural_Development_Study_No2/IO/output.csv/_temporary/0/_temporary/attempt_202109091118556601553846925071241_0051_m_000033_2233 (exists=false, cwd=file:/home/hadoop/.workspace/app-20210909111836-0003/0)\n",
      "\tat org.apache.hadoop.fs.ChecksumFileSystem.create(ChecksumFileSystem.java:458)\n",
      "\tat org.apache.hadoop.fs.ChecksumFileSystem.create(ChecksumFileSystem.java:443)\n",
      "\tat org.apache.hadoop.fs.FileSystem.create(FileSystem.java:1118)\n",
      "\tat org.apache.hadoop.fs.FileSystem.create(FileSystem.java:1098)\n",
      "\tat org.apache.hadoop.fs.FileSystem.create(FileSystem.java:987)\n",
      "\tat org.apache.spark.sql.execution.datasources.CodecStreams$.createOutputStream(CodecStreams.scala:81)\n",
      "\tat org.apache.spark.sql.execution.datasources.CodecStreams$.createOutputStreamWriter(CodecStreams.scala:92)\n",
      "\tat org.apache.spark.sql.execution.datasources.csv.CsvOutputWriter.<init>(CsvOutputWriter.scala:38)\n",
      "\tat org.apache.spark.sql.execution.datasources.csv.CSVFileFormat$$anon$1.newInstance(CSVFileFormat.scala:84)\n",
      "\tat org.apache.spark.sql.execution.datasources.SingleDirectoryDataWriter.newOutputWriter(FileFormatDataWriter.scala:126)\n",
      "\tat org.apache.spark.sql.execution.datasources.SingleDirectoryDataWriter.<init>(FileFormatDataWriter.scala:111)\n",
      "\tat org.apache.spark.sql.execution.datasources.FileFormatWriter$.executeTask(FileFormatWriter.scala:269)\n",
      "\tat org.apache.spark.sql.execution.datasources.FileFormatWriter$.$anonfun$write$15(FileFormatWriter.scala:210)\n",
      "\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:90)\n",
      "\tat org.apache.spark.scheduler.Task.run(Task.scala:131)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$3(Executor.scala:497)\n",
      "\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1439)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:500)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n",
      "\tat java.lang.Thread.run(Thread.java:748)\n",
      "\n",
      "21/09/09 11:18:56 WARN TaskSetManager: Lost task 22.0 in stage 51.0 (TID 2232) (192.168.1.2 executor 0): java.io.IOException: Mkdirs failed to create file:/home/geno1664/Developments/Github_Samples/RDS-ENV/Rural_Development_Study_No2/IO/output.csv/_temporary/0/_temporary/attempt_20210909111855776614265399591411_0051_m_000022_2232 (exists=false, cwd=file:/home/hadoop/.workspace/app-20210909111836-0003/0)\n",
      "\tat org.apache.hadoop.fs.ChecksumFileSystem.create(ChecksumFileSystem.java:458)\n",
      "\tat org.apache.hadoop.fs.ChecksumFileSystem.create(ChecksumFileSystem.java:443)\n",
      "\tat org.apache.hadoop.fs.FileSystem.create(FileSystem.java:1118)\n",
      "\tat org.apache.hadoop.fs.FileSystem.create(FileSystem.java:1098)\n",
      "\tat org.apache.hadoop.fs.FileSystem.create(FileSystem.java:987)\n",
      "\tat org.apache.spark.sql.execution.datasources.CodecStreams$.createOutputStream(CodecStreams.scala:81)\n",
      "\tat org.apache.spark.sql.execution.datasources.CodecStreams$.createOutputStreamWriter(CodecStreams.scala:92)\n",
      "\tat org.apache.spark.sql.execution.datasources.csv.CsvOutputWriter.<init>(CsvOutputWriter.scala:38)\n",
      "\tat org.apache.spark.sql.execution.datasources.csv.CSVFileFormat$$anon$1.newInstance(CSVFileFormat.scala:84)\n",
      "\tat org.apache.spark.sql.execution.datasources.SingleDirectoryDataWriter.newOutputWriter(FileFormatDataWriter.scala:126)\n",
      "\tat org.apache.spark.sql.execution.datasources.SingleDirectoryDataWriter.<init>(FileFormatDataWriter.scala:111)\n",
      "\tat org.apache.spark.sql.execution.datasources.FileFormatWriter$.executeTask(FileFormatWriter.scala:269)\n",
      "\tat org.apache.spark.sql.execution.datasources.FileFormatWriter$.$anonfun$write$15(FileFormatWriter.scala:210)\n",
      "\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:90)\n",
      "\tat org.apache.spark.scheduler.Task.run(Task.scala:131)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$3(Executor.scala:497)\n",
      "\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1439)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:500)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n",
      "\tat java.lang.Thread.run(Thread.java:748)\n",
      "\n",
      "21/09/09 11:18:56 WARN TaskSetManager: Lost task 0.0 in stage 51.0 (TID 2239) (192.168.1.2 executor 0): java.io.IOException: Mkdirs failed to create file:/home/geno1664/Developments/Github_Samples/RDS-ENV/Rural_Development_Study_No2/IO/output.csv/_temporary/0/_temporary/attempt_202109091118557133287163477300009_0051_m_000000_2239 (exists=false, cwd=file:/home/hadoop/.workspace/app-20210909111836-0003/0)\n",
      "\tat org.apache.hadoop.fs.ChecksumFileSystem.create(ChecksumFileSystem.java:458)\n",
      "\tat org.apache.hadoop.fs.ChecksumFileSystem.create(ChecksumFileSystem.java:443)\n",
      "\tat org.apache.hadoop.fs.FileSystem.create(FileSystem.java:1118)\n",
      "\tat org.apache.hadoop.fs.FileSystem.create(FileSystem.java:1098)\n",
      "\tat org.apache.hadoop.fs.FileSystem.create(FileSystem.java:987)\n",
      "\tat org.apache.spark.sql.execution.datasources.CodecStreams$.createOutputStream(CodecStreams.scala:81)\n",
      "\tat org.apache.spark.sql.execution.datasources.CodecStreams$.createOutputStreamWriter(CodecStreams.scala:92)\n",
      "\tat org.apache.spark.sql.execution.datasources.csv.CsvOutputWriter.<init>(CsvOutputWriter.scala:38)\n",
      "\tat org.apache.spark.sql.execution.datasources.csv.CSVFileFormat$$anon$1.newInstance(CSVFileFormat.scala:84)\n",
      "\tat org.apache.spark.sql.execution.datasources.SingleDirectoryDataWriter.newOutputWriter(FileFormatDataWriter.scala:126)\n",
      "\tat org.apache.spark.sql.execution.datasources.SingleDirectoryDataWriter.<init>(FileFormatDataWriter.scala:111)\n",
      "\tat org.apache.spark.sql.execution.datasources.FileFormatWriter$.executeTask(FileFormatWriter.scala:269)\n",
      "\tat org.apache.spark.sql.execution.datasources.FileFormatWriter$.$anonfun$write$15(FileFormatWriter.scala:210)\n",
      "\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:90)\n",
      "\tat org.apache.spark.scheduler.Task.run(Task.scala:131)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$3(Executor.scala:497)\n",
      "\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1439)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:500)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n",
      "\tat java.lang.Thread.run(Thread.java:748)\n",
      "\n",
      "21/09/09 11:18:56 WARN TaskSetManager: Lost task 169.0 in stage 51.0 (TID 2238) (192.168.1.2 executor 0): java.io.IOException: Mkdirs failed to create file:/home/geno1664/Developments/Github_Samples/RDS-ENV/Rural_Development_Study_No2/IO/output.csv/_temporary/0/_temporary/attempt_202109091118558157999164613245380_0051_m_000169_2238 (exists=false, cwd=file:/home/hadoop/.workspace/app-20210909111836-0003/0)\n",
      "\tat org.apache.hadoop.fs.ChecksumFileSystem.create(ChecksumFileSystem.java:458)\n",
      "\tat org.apache.hadoop.fs.ChecksumFileSystem.create(ChecksumFileSystem.java:443)\n",
      "\tat org.apache.hadoop.fs.FileSystem.create(FileSystem.java:1118)\n",
      "\tat org.apache.hadoop.fs.FileSystem.create(FileSystem.java:1098)\n",
      "\tat org.apache.hadoop.fs.FileSystem.create(FileSystem.java:987)\n",
      "\tat org.apache.spark.sql.execution.datasources.CodecStreams$.createOutputStream(CodecStreams.scala:81)\n",
      "\tat org.apache.spark.sql.execution.datasources.CodecStreams$.createOutputStreamWriter(CodecStreams.scala:92)\n",
      "\tat org.apache.spark.sql.execution.datasources.csv.CsvOutputWriter.<init>(CsvOutputWriter.scala:38)\n",
      "\tat org.apache.spark.sql.execution.datasources.csv.CSVFileFormat$$anon$1.newInstance(CSVFileFormat.scala:84)\n",
      "\tat org.apache.spark.sql.execution.datasources.SingleDirectoryDataWriter.newOutputWriter(FileFormatDataWriter.scala:126)\n",
      "\tat org.apache.spark.sql.execution.datasources.SingleDirectoryDataWriter.<init>(FileFormatDataWriter.scala:111)\n",
      "\tat org.apache.spark.sql.execution.datasources.FileFormatWriter$.executeTask(FileFormatWriter.scala:269)\n",
      "\tat org.apache.spark.sql.execution.datasources.FileFormatWriter$.$anonfun$write$15(FileFormatWriter.scala:210)\n",
      "\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:90)\n",
      "\tat org.apache.spark.scheduler.Task.run(Task.scala:131)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$3(Executor.scala:497)\n",
      "\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1439)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:500)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n",
      "\tat java.lang.Thread.run(Thread.java:748)\n",
      "\n",
      "21/09/09 11:18:56 WARN TaskSetManager: Lost task 33.1 in stage 51.0 (TID 2246) (192.168.1.2 executor 0): java.io.IOException: Mkdirs failed to create file:/home/geno1664/Developments/Github_Samples/RDS-ENV/Rural_Development_Study_No2/IO/output.csv/_temporary/0/_temporary/attempt_20210909111855430814963959183879_0051_m_000033_2246 (exists=false, cwd=file:/home/hadoop/.workspace/app-20210909111836-0003/0)\n",
      "\tat org.apache.hadoop.fs.ChecksumFileSystem.create(ChecksumFileSystem.java:458)\n",
      "\tat org.apache.hadoop.fs.ChecksumFileSystem.create(ChecksumFileSystem.java:443)\n",
      "\tat org.apache.hadoop.fs.FileSystem.create(FileSystem.java:1118)\n",
      "\tat org.apache.hadoop.fs.FileSystem.create(FileSystem.java:1098)\n",
      "\tat org.apache.hadoop.fs.FileSystem.create(FileSystem.java:987)\n",
      "\tat org.apache.spark.sql.execution.datasources.CodecStreams$.createOutputStream(CodecStreams.scala:81)\n",
      "\tat org.apache.spark.sql.execution.datasources.CodecStreams$.createOutputStreamWriter(CodecStreams.scala:92)\n",
      "\tat org.apache.spark.sql.execution.datasources.csv.CsvOutputWriter.<init>(CsvOutputWriter.scala:38)\n",
      "\tat org.apache.spark.sql.execution.datasources.csv.CSVFileFormat$$anon$1.newInstance(CSVFileFormat.scala:84)\n",
      "\tat org.apache.spark.sql.execution.datasources.SingleDirectoryDataWriter.newOutputWriter(FileFormatDataWriter.scala:126)\n",
      "\tat org.apache.spark.sql.execution.datasources.SingleDirectoryDataWriter.<init>(FileFormatDataWriter.scala:111)\n",
      "\tat org.apache.spark.sql.execution.datasources.FileFormatWriter$.executeTask(FileFormatWriter.scala:269)\n",
      "\tat org.apache.spark.sql.execution.datasources.FileFormatWriter$.$anonfun$write$15(FileFormatWriter.scala:210)\n",
      "\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:90)\n",
      "\tat org.apache.spark.scheduler.Task.run(Task.scala:131)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$3(Executor.scala:497)\n",
      "\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1439)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:500)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n",
      "\tat java.lang.Thread.run(Thread.java:748)\n",
      "\n",
      "21/09/09 11:18:56 WARN TaskSetManager: Lost task 130.1 in stage 51.0 (TID 2248) (192.168.1.2 executor 0): java.io.IOException: Mkdirs failed to create file:/home/geno1664/Developments/Github_Samples/RDS-ENV/Rural_Development_Study_No2/IO/output.csv/_temporary/0/_temporary/attempt_202109091118553840614313692926942_0051_m_000130_2248 (exists=false, cwd=file:/home/hadoop/.workspace/app-20210909111836-0003/0)\n",
      "\tat org.apache.hadoop.fs.ChecksumFileSystem.create(ChecksumFileSystem.java:458)\n",
      "\tat org.apache.hadoop.fs.ChecksumFileSystem.create(ChecksumFileSystem.java:443)\n",
      "\tat org.apache.hadoop.fs.FileSystem.create(FileSystem.java:1118)\n",
      "\tat org.apache.hadoop.fs.FileSystem.create(FileSystem.java:1098)\n",
      "\tat org.apache.hadoop.fs.FileSystem.create(FileSystem.java:987)\n",
      "\tat org.apache.spark.sql.execution.datasources.CodecStreams$.createOutputStream(CodecStreams.scala:81)\n",
      "\tat org.apache.spark.sql.execution.datasources.CodecStreams$.createOutputStreamWriter(CodecStreams.scala:92)\n",
      "\tat org.apache.spark.sql.execution.datasources.csv.CsvOutputWriter.<init>(CsvOutputWriter.scala:38)\n",
      "\tat org.apache.spark.sql.execution.datasources.csv.CSVFileFormat$$anon$1.newInstance(CSVFileFormat.scala:84)\n",
      "\tat org.apache.spark.sql.execution.datasources.SingleDirectoryDataWriter.newOutputWriter(FileFormatDataWriter.scala:126)\n",
      "\tat org.apache.spark.sql.execution.datasources.SingleDirectoryDataWriter.<init>(FileFormatDataWriter.scala:111)\n",
      "\tat org.apache.spark.sql.execution.datasources.FileFormatWriter$.executeTask(FileFormatWriter.scala:269)\n",
      "\tat org.apache.spark.sql.execution.datasources.FileFormatWriter$.$anonfun$write$15(FileFormatWriter.scala:210)\n",
      "\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:90)\n",
      "\tat org.apache.spark.scheduler.Task.run(Task.scala:131)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$3(Executor.scala:497)\n",
      "\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1439)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:500)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n",
      "\tat java.lang.Thread.run(Thread.java:748)\n",
      "\n",
      "21/09/09 11:18:56 WARN TaskSetManager: Lost task 22.1 in stage 51.0 (TID 2247) (192.168.1.2 executor 0): java.io.IOException: Mkdirs failed to create file:/home/geno1664/Developments/Github_Samples/RDS-ENV/Rural_Development_Study_No2/IO/output.csv/_temporary/0/_temporary/attempt_202109091118552913263018275421229_0051_m_000022_2247 (exists=false, cwd=file:/home/hadoop/.workspace/app-20210909111836-0003/0)\n",
      "\tat org.apache.hadoop.fs.ChecksumFileSystem.create(ChecksumFileSystem.java:458)\n",
      "\tat org.apache.hadoop.fs.ChecksumFileSystem.create(ChecksumFileSystem.java:443)\n",
      "\tat org.apache.hadoop.fs.FileSystem.create(FileSystem.java:1118)\n",
      "\tat org.apache.hadoop.fs.FileSystem.create(FileSystem.java:1098)\n",
      "\tat org.apache.hadoop.fs.FileSystem.create(FileSystem.java:987)\n",
      "\tat org.apache.spark.sql.execution.datasources.CodecStreams$.createOutputStream(CodecStreams.scala:81)\n",
      "\tat org.apache.spark.sql.execution.datasources.CodecStreams$.createOutputStreamWriter(CodecStreams.scala:92)\n",
      "\tat org.apache.spark.sql.execution.datasources.csv.CsvOutputWriter.<init>(CsvOutputWriter.scala:38)\n",
      "\tat org.apache.spark.sql.execution.datasources.csv.CSVFileFormat$$anon$1.newInstance(CSVFileFormat.scala:84)\n",
      "\tat org.apache.spark.sql.execution.datasources.SingleDirectoryDataWriter.newOutputWriter(FileFormatDataWriter.scala:126)\n",
      "\tat org.apache.spark.sql.execution.datasources.SingleDirectoryDataWriter.<init>(FileFormatDataWriter.scala:111)\n",
      "\tat org.apache.spark.sql.execution.datasources.FileFormatWriter$.executeTask(FileFormatWriter.scala:269)\n",
      "\tat org.apache.spark.sql.execution.datasources.FileFormatWriter$.$anonfun$write$15(FileFormatWriter.scala:210)\n",
      "\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:90)\n",
      "\tat org.apache.spark.scheduler.Task.run(Task.scala:131)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$3(Executor.scala:497)\n",
      "\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1439)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:500)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n",
      "\tat java.lang.Thread.run(Thread.java:748)\n",
      "\n",
      "21/09/09 11:18:56 WARN TaskSetManager: Lost task 8.1 in stage 51.0 (TID 2250) (192.168.1.2 executor 0): java.io.IOException: Mkdirs failed to create file:/home/geno1664/Developments/Github_Samples/RDS-ENV/Rural_Development_Study_No2/IO/output.csv/_temporary/0/_temporary/attempt_202109091118553469829630454957307_0051_m_000008_2250 (exists=false, cwd=file:/home/hadoop/.workspace/app-20210909111836-0003/0)\n",
      "\tat org.apache.hadoop.fs.ChecksumFileSystem.create(ChecksumFileSystem.java:458)\n",
      "\tat org.apache.hadoop.fs.ChecksumFileSystem.create(ChecksumFileSystem.java:443)\n",
      "\tat org.apache.hadoop.fs.FileSystem.create(FileSystem.java:1118)\n",
      "\tat org.apache.hadoop.fs.FileSystem.create(FileSystem.java:1098)\n",
      "\tat org.apache.hadoop.fs.FileSystem.create(FileSystem.java:987)\n",
      "\tat org.apache.spark.sql.execution.datasources.CodecStreams$.createOutputStream(CodecStreams.scala:81)\n",
      "\tat org.apache.spark.sql.execution.datasources.CodecStreams$.createOutputStreamWriter(CodecStreams.scala:92)\n",
      "\tat org.apache.spark.sql.execution.datasources.csv.CsvOutputWriter.<init>(CsvOutputWriter.scala:38)\n",
      "\tat org.apache.spark.sql.execution.datasources.csv.CSVFileFormat$$anon$1.newInstance(CSVFileFormat.scala:84)\n",
      "\tat org.apache.spark.sql.execution.datasources.SingleDirectoryDataWriter.newOutputWriter(FileFormatDataWriter.scala:126)\n",
      "\tat org.apache.spark.sql.execution.datasources.SingleDirectoryDataWriter.<init>(FileFormatDataWriter.scala:111)\n",
      "\tat org.apache.spark.sql.execution.datasources.FileFormatWriter$.executeTask(FileFormatWriter.scala:269)\n",
      "\tat org.apache.spark.sql.execution.datasources.FileFormatWriter$.$anonfun$write$15(FileFormatWriter.scala:210)\n",
      "\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:90)\n",
      "\tat org.apache.spark.scheduler.Task.run(Task.scala:131)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$3(Executor.scala:497)\n",
      "\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1439)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:500)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n",
      "\tat java.lang.Thread.run(Thread.java:748)\n",
      "\n",
      "21/09/09 11:18:56 WARN TaskSetManager: Lost task 11.1 in stage 51.0 (TID 2253) (192.168.1.2 executor 0): java.io.IOException: Mkdirs failed to create file:/home/geno1664/Developments/Github_Samples/RDS-ENV/Rural_Development_Study_No2/IO/output.csv/_temporary/0/_temporary/attempt_202109091118557015015322921698698_0051_m_000011_2253 (exists=false, cwd=file:/home/hadoop/.workspace/app-20210909111836-0003/0)\n",
      "\tat org.apache.hadoop.fs.ChecksumFileSystem.create(ChecksumFileSystem.java:458)\n",
      "\tat org.apache.hadoop.fs.ChecksumFileSystem.create(ChecksumFileSystem.java:443)\n",
      "\tat org.apache.hadoop.fs.FileSystem.create(FileSystem.java:1118)\n",
      "\tat org.apache.hadoop.fs.FileSystem.create(FileSystem.java:1098)\n",
      "\tat org.apache.hadoop.fs.FileSystem.create(FileSystem.java:987)\n",
      "\tat org.apache.spark.sql.execution.datasources.CodecStreams$.createOutputStream(CodecStreams.scala:81)\n",
      "\tat org.apache.spark.sql.execution.datasources.CodecStreams$.createOutputStreamWriter(CodecStreams.scala:92)\n",
      "\tat org.apache.spark.sql.execution.datasources.csv.CsvOutputWriter.<init>(CsvOutputWriter.scala:38)\n",
      "\tat org.apache.spark.sql.execution.datasources.csv.CSVFileFormat$$anon$1.newInstance(CSVFileFormat.scala:84)\n",
      "\tat org.apache.spark.sql.execution.datasources.SingleDirectoryDataWriter.newOutputWriter(FileFormatDataWriter.scala:126)\n",
      "\tat org.apache.spark.sql.execution.datasources.SingleDirectoryDataWriter.<init>(FileFormatDataWriter.scala:111)\n",
      "\tat org.apache.spark.sql.execution.datasources.FileFormatWriter$.executeTask(FileFormatWriter.scala:269)\n",
      "\tat org.apache.spark.sql.execution.datasources.FileFormatWriter$.$anonfun$write$15(FileFormatWriter.scala:210)\n",
      "\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:90)\n",
      "\tat org.apache.spark.scheduler.Task.run(Task.scala:131)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$3(Executor.scala:497)\n",
      "\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1439)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:500)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n",
      "\tat java.lang.Thread.run(Thread.java:748)\n",
      "\n",
      "21/09/09 11:18:56 WARN TaskSetManager: Lost task 79.1 in stage 51.0 (TID 2249) (192.168.1.2 executor 0): java.io.IOException: Mkdirs failed to create file:/home/geno1664/Developments/Github_Samples/RDS-ENV/Rural_Development_Study_No2/IO/output.csv/_temporary/0/_temporary/attempt_202109091118551734497840978133082_0051_m_000079_2249 (exists=false, cwd=file:/home/hadoop/.workspace/app-20210909111836-0003/0)\n",
      "\tat org.apache.hadoop.fs.ChecksumFileSystem.create(ChecksumFileSystem.java:458)\n",
      "\tat org.apache.hadoop.fs.ChecksumFileSystem.create(ChecksumFileSystem.java:443)\n",
      "\tat org.apache.hadoop.fs.FileSystem.create(FileSystem.java:1118)\n",
      "\tat org.apache.hadoop.fs.FileSystem.create(FileSystem.java:1098)\n",
      "\tat org.apache.hadoop.fs.FileSystem.create(FileSystem.java:987)\n",
      "\tat org.apache.spark.sql.execution.datasources.CodecStreams$.createOutputStream(CodecStreams.scala:81)\n",
      "\tat org.apache.spark.sql.execution.datasources.CodecStreams$.createOutputStreamWriter(CodecStreams.scala:92)\n",
      "\tat org.apache.spark.sql.execution.datasources.csv.CsvOutputWriter.<init>(CsvOutputWriter.scala:38)\n",
      "\tat org.apache.spark.sql.execution.datasources.csv.CSVFileFormat$$anon$1.newInstance(CSVFileFormat.scala:84)\n",
      "\tat org.apache.spark.sql.execution.datasources.SingleDirectoryDataWriter.newOutputWriter(FileFormatDataWriter.scala:126)\n",
      "\tat org.apache.spark.sql.execution.datasources.SingleDirectoryDataWriter.<init>(FileFormatDataWriter.scala:111)\n",
      "\tat org.apache.spark.sql.execution.datasources.FileFormatWriter$.executeTask(FileFormatWriter.scala:269)\n",
      "\tat org.apache.spark.sql.execution.datasources.FileFormatWriter$.$anonfun$write$15(FileFormatWriter.scala:210)\n",
      "\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:90)\n",
      "\tat org.apache.spark.scheduler.Task.run(Task.scala:131)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$3(Executor.scala:497)\n",
      "\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1439)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:500)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n",
      "\tat java.lang.Thread.run(Thread.java:748)\n",
      "\n",
      "21/09/09 11:18:56 WARN TaskSetManager: Lost task 103.1 in stage 51.0 (TID 2252) (192.168.1.2 executor 0): java.io.IOException: Mkdirs failed to create file:/home/geno1664/Developments/Github_Samples/RDS-ENV/Rural_Development_Study_No2/IO/output.csv/_temporary/0/_temporary/attempt_202109091118557339559301645147651_0051_m_000103_2252 (exists=false, cwd=file:/home/hadoop/.workspace/app-20210909111836-0003/0)\n",
      "\tat org.apache.hadoop.fs.ChecksumFileSystem.create(ChecksumFileSystem.java:458)\n",
      "\tat org.apache.hadoop.fs.ChecksumFileSystem.create(ChecksumFileSystem.java:443)\n",
      "\tat org.apache.hadoop.fs.FileSystem.create(FileSystem.java:1118)\n",
      "\tat org.apache.hadoop.fs.FileSystem.create(FileSystem.java:1098)\n",
      "\tat org.apache.hadoop.fs.FileSystem.create(FileSystem.java:987)\n",
      "\tat org.apache.spark.sql.execution.datasources.CodecStreams$.createOutputStream(CodecStreams.scala:81)\n",
      "\tat org.apache.spark.sql.execution.datasources.CodecStreams$.createOutputStreamWriter(CodecStreams.scala:92)\n",
      "\tat org.apache.spark.sql.execution.datasources.csv.CsvOutputWriter.<init>(CsvOutputWriter.scala:38)\n",
      "\tat org.apache.spark.sql.execution.datasources.csv.CSVFileFormat$$anon$1.newInstance(CSVFileFormat.scala:84)\n",
      "\tat org.apache.spark.sql.execution.datasources.SingleDirectoryDataWriter.newOutputWriter(FileFormatDataWriter.scala:126)\n",
      "\tat org.apache.spark.sql.execution.datasources.SingleDirectoryDataWriter.<init>(FileFormatDataWriter.scala:111)\n",
      "\tat org.apache.spark.sql.execution.datasources.FileFormatWriter$.executeTask(FileFormatWriter.scala:269)\n",
      "\tat org.apache.spark.sql.execution.datasources.FileFormatWriter$.$anonfun$write$15(FileFormatWriter.scala:210)\n",
      "\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:90)\n",
      "\tat org.apache.spark.scheduler.Task.run(Task.scala:131)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$3(Executor.scala:497)\n",
      "\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1439)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:500)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n",
      "\tat java.lang.Thread.run(Thread.java:748)\n",
      "\n",
      "21/09/09 11:18:56 WARN TaskSetManager: Lost task 0.1 in stage 51.0 (TID 2256) (192.168.1.2 executor 0): java.io.IOException: Mkdirs failed to create file:/home/geno1664/Developments/Github_Samples/RDS-ENV/Rural_Development_Study_No2/IO/output.csv/_temporary/0/_temporary/attempt_202109091118557865761442367226583_0051_m_000000_2256 (exists=false, cwd=file:/home/hadoop/.workspace/app-20210909111836-0003/0)\n",
      "\tat org.apache.hadoop.fs.ChecksumFileSystem.create(ChecksumFileSystem.java:458)\n",
      "\tat org.apache.hadoop.fs.ChecksumFileSystem.create(ChecksumFileSystem.java:443)\n",
      "\tat org.apache.hadoop.fs.FileSystem.create(FileSystem.java:1118)\n",
      "\tat org.apache.hadoop.fs.FileSystem.create(FileSystem.java:1098)\n",
      "\tat org.apache.hadoop.fs.FileSystem.create(FileSystem.java:987)\n",
      "\tat org.apache.spark.sql.execution.datasources.CodecStreams$.createOutputStream(CodecStreams.scala:81)\n",
      "\tat org.apache.spark.sql.execution.datasources.CodecStreams$.createOutputStreamWriter(CodecStreams.scala:92)\n",
      "\tat org.apache.spark.sql.execution.datasources.csv.CsvOutputWriter.<init>(CsvOutputWriter.scala:38)\n",
      "\tat org.apache.spark.sql.execution.datasources.csv.CSVFileFormat$$anon$1.newInstance(CSVFileFormat.scala:84)\n",
      "\tat org.apache.spark.sql.execution.datasources.SingleDirectoryDataWriter.newOutputWriter(FileFormatDataWriter.scala:126)\n",
      "\tat org.apache.spark.sql.execution.datasources.SingleDirectoryDataWriter.<init>(FileFormatDataWriter.scala:111)\n",
      "\tat org.apache.spark.sql.execution.datasources.FileFormatWriter$.executeTask(FileFormatWriter.scala:269)\n",
      "\tat org.apache.spark.sql.execution.datasources.FileFormatWriter$.$anonfun$write$15(FileFormatWriter.scala:210)\n",
      "\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:90)\n",
      "\tat org.apache.spark.scheduler.Task.run(Task.scala:131)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$3(Executor.scala:497)\n",
      "\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1439)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:500)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n",
      "\tat java.lang.Thread.run(Thread.java:748)\n",
      "\n",
      "21/09/09 11:18:56 WARN TaskSetManager: Lost task 169.1 in stage 51.0 (TID 2251) (192.168.1.2 executor 0): java.io.IOException: Mkdirs failed to create file:/home/geno1664/Developments/Github_Samples/RDS-ENV/Rural_Development_Study_No2/IO/output.csv/_temporary/0/_temporary/attempt_202109091118553992809683154544609_0051_m_000169_2251 (exists=false, cwd=file:/home/hadoop/.workspace/app-20210909111836-0003/0)\n",
      "\tat org.apache.hadoop.fs.ChecksumFileSystem.create(ChecksumFileSystem.java:458)\n",
      "\tat org.apache.hadoop.fs.ChecksumFileSystem.create(ChecksumFileSystem.java:443)\n",
      "\tat org.apache.hadoop.fs.FileSystem.create(FileSystem.java:1118)\n",
      "\tat org.apache.hadoop.fs.FileSystem.create(FileSystem.java:1098)\n",
      "\tat org.apache.hadoop.fs.FileSystem.create(FileSystem.java:987)\n",
      "\tat org.apache.spark.sql.execution.datasources.CodecStreams$.createOutputStream(CodecStreams.scala:81)\n",
      "\tat org.apache.spark.sql.execution.datasources.CodecStreams$.createOutputStreamWriter(CodecStreams.scala:92)\n",
      "\tat org.apache.spark.sql.execution.datasources.csv.CsvOutputWriter.<init>(CsvOutputWriter.scala:38)\n",
      "\tat org.apache.spark.sql.execution.datasources.csv.CSVFileFormat$$anon$1.newInstance(CSVFileFormat.scala:84)\n",
      "\tat org.apache.spark.sql.execution.datasources.SingleDirectoryDataWriter.newOutputWriter(FileFormatDataWriter.scala:126)\n",
      "\tat org.apache.spark.sql.execution.datasources.SingleDirectoryDataWriter.<init>(FileFormatDataWriter.scala:111)\n",
      "\tat org.apache.spark.sql.execution.datasources.FileFormatWriter$.executeTask(FileFormatWriter.scala:269)\n",
      "\tat org.apache.spark.sql.execution.datasources.FileFormatWriter$.$anonfun$write$15(FileFormatWriter.scala:210)\n",
      "\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:90)\n",
      "\tat org.apache.spark.scheduler.Task.run(Task.scala:131)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$3(Executor.scala:497)\n",
      "\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1439)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:500)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n",
      "\tat java.lang.Thread.run(Thread.java:748)\n",
      "\n",
      "21/09/09 11:18:56 WARN TaskSetManager: Lost task 59.1 in stage 51.0 (TID 2255) (192.168.1.2 executor 0): java.io.IOException: Mkdirs failed to create file:/home/geno1664/Developments/Github_Samples/RDS-ENV/Rural_Development_Study_No2/IO/output.csv/_temporary/0/_temporary/attempt_202109091118555338099016078716313_0051_m_000059_2255 (exists=false, cwd=file:/home/hadoop/.workspace/app-20210909111836-0003/0)\n",
      "\tat org.apache.hadoop.fs.ChecksumFileSystem.create(ChecksumFileSystem.java:458)\n",
      "\tat org.apache.hadoop.fs.ChecksumFileSystem.create(ChecksumFileSystem.java:443)\n",
      "\tat org.apache.hadoop.fs.FileSystem.create(FileSystem.java:1118)\n",
      "\tat org.apache.hadoop.fs.FileSystem.create(FileSystem.java:1098)\n",
      "\tat org.apache.hadoop.fs.FileSystem.create(FileSystem.java:987)\n",
      "\tat org.apache.spark.sql.execution.datasources.CodecStreams$.createOutputStream(CodecStreams.scala:81)\n",
      "\tat org.apache.spark.sql.execution.datasources.CodecStreams$.createOutputStreamWriter(CodecStreams.scala:92)\n",
      "\tat org.apache.spark.sql.execution.datasources.csv.CsvOutputWriter.<init>(CsvOutputWriter.scala:38)\n",
      "\tat org.apache.spark.sql.execution.datasources.csv.CSVFileFormat$$anon$1.newInstance(CSVFileFormat.scala:84)\n",
      "\tat org.apache.spark.sql.execution.datasources.SingleDirectoryDataWriter.newOutputWriter(FileFormatDataWriter.scala:126)\n",
      "\tat org.apache.spark.sql.execution.datasources.SingleDirectoryDataWriter.<init>(FileFormatDataWriter.scala:111)\n",
      "\tat org.apache.spark.sql.execution.datasources.FileFormatWriter$.executeTask(FileFormatWriter.scala:269)\n",
      "\tat org.apache.spark.sql.execution.datasources.FileFormatWriter$.$anonfun$write$15(FileFormatWriter.scala:210)\n",
      "\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:90)\n",
      "\tat org.apache.spark.scheduler.Task.run(Task.scala:131)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$3(Executor.scala:497)\n",
      "\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1439)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:500)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n",
      "\tat java.lang.Thread.run(Thread.java:748)\n",
      "\n",
      "21/09/09 11:18:56 WARN TaskSetManager: Lost task 33.2 in stage 51.0 (TID 2254) (192.168.1.2 executor 0): java.io.IOException: Mkdirs failed to create file:/home/geno1664/Developments/Github_Samples/RDS-ENV/Rural_Development_Study_No2/IO/output.csv/_temporary/0/_temporary/attempt_202109091118555963328406188301104_0051_m_000033_2254 (exists=false, cwd=file:/home/hadoop/.workspace/app-20210909111836-0003/0)\n",
      "\tat org.apache.hadoop.fs.ChecksumFileSystem.create(ChecksumFileSystem.java:458)\n",
      "\tat org.apache.hadoop.fs.ChecksumFileSystem.create(ChecksumFileSystem.java:443)\n",
      "\tat org.apache.hadoop.fs.FileSystem.create(FileSystem.java:1118)\n",
      "\tat org.apache.hadoop.fs.FileSystem.create(FileSystem.java:1098)\n",
      "\tat org.apache.hadoop.fs.FileSystem.create(FileSystem.java:987)\n",
      "\tat org.apache.spark.sql.execution.datasources.CodecStreams$.createOutputStream(CodecStreams.scala:81)\n",
      "\tat org.apache.spark.sql.execution.datasources.CodecStreams$.createOutputStreamWriter(CodecStreams.scala:92)\n",
      "\tat org.apache.spark.sql.execution.datasources.csv.CsvOutputWriter.<init>(CsvOutputWriter.scala:38)\n",
      "\tat org.apache.spark.sql.execution.datasources.csv.CSVFileFormat$$anon$1.newInstance(CSVFileFormat.scala:84)\n",
      "\tat org.apache.spark.sql.execution.datasources.SingleDirectoryDataWriter.newOutputWriter(FileFormatDataWriter.scala:126)\n",
      "\tat org.apache.spark.sql.execution.datasources.SingleDirectoryDataWriter.<init>(FileFormatDataWriter.scala:111)\n",
      "\tat org.apache.spark.sql.execution.datasources.FileFormatWriter$.executeTask(FileFormatWriter.scala:269)\n",
      "\tat org.apache.spark.sql.execution.datasources.FileFormatWriter$.$anonfun$write$15(FileFormatWriter.scala:210)\n",
      "\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:90)\n",
      "\tat org.apache.spark.scheduler.Task.run(Task.scala:131)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$3(Executor.scala:497)\n",
      "\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1439)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:500)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n",
      "\tat java.lang.Thread.run(Thread.java:748)\n",
      "\n",
      "21/09/09 11:18:56 WARN TaskSetManager: Lost task 130.2 in stage 51.0 (TID 2258) (192.168.1.2 executor 0): java.io.IOException: Mkdirs failed to create file:/home/geno1664/Developments/Github_Samples/RDS-ENV/Rural_Development_Study_No2/IO/output.csv/_temporary/0/_temporary/attempt_202109091118557242796002335601056_0051_m_000130_2258 (exists=false, cwd=file:/home/hadoop/.workspace/app-20210909111836-0003/0)\n",
      "\tat org.apache.hadoop.fs.ChecksumFileSystem.create(ChecksumFileSystem.java:458)\n",
      "\tat org.apache.hadoop.fs.ChecksumFileSystem.create(ChecksumFileSystem.java:443)\n",
      "\tat org.apache.hadoop.fs.FileSystem.create(FileSystem.java:1118)\n",
      "\tat org.apache.hadoop.fs.FileSystem.create(FileSystem.java:1098)\n",
      "\tat org.apache.hadoop.fs.FileSystem.create(FileSystem.java:987)\n",
      "\tat org.apache.spark.sql.execution.datasources.CodecStreams$.createOutputStream(CodecStreams.scala:81)\n",
      "\tat org.apache.spark.sql.execution.datasources.CodecStreams$.createOutputStreamWriter(CodecStreams.scala:92)\n",
      "\tat org.apache.spark.sql.execution.datasources.csv.CsvOutputWriter.<init>(CsvOutputWriter.scala:38)\n",
      "\tat org.apache.spark.sql.execution.datasources.csv.CSVFileFormat$$anon$1.newInstance(CSVFileFormat.scala:84)\n",
      "\tat org.apache.spark.sql.execution.datasources.SingleDirectoryDataWriter.newOutputWriter(FileFormatDataWriter.scala:126)\n",
      "\tat org.apache.spark.sql.execution.datasources.SingleDirectoryDataWriter.<init>(FileFormatDataWriter.scala:111)\n",
      "\tat org.apache.spark.sql.execution.datasources.FileFormatWriter$.executeTask(FileFormatWriter.scala:269)\n",
      "\tat org.apache.spark.sql.execution.datasources.FileFormatWriter$.$anonfun$write$15(FileFormatWriter.scala:210)\n",
      "\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:90)\n",
      "\tat org.apache.spark.scheduler.Task.run(Task.scala:131)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$3(Executor.scala:497)\n",
      "\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1439)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:500)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n",
      "\tat java.lang.Thread.run(Thread.java:748)\n",
      "\n",
      "21/09/09 11:18:56 WARN TaskSetManager: Lost task 22.2 in stage 51.0 (TID 2257) (192.168.1.2 executor 0): java.io.IOException: Mkdirs failed to create file:/home/geno1664/Developments/Github_Samples/RDS-ENV/Rural_Development_Study_No2/IO/output.csv/_temporary/0/_temporary/attempt_202109091118558472819553146412623_0051_m_000022_2257 (exists=false, cwd=file:/home/hadoop/.workspace/app-20210909111836-0003/0)\n",
      "\tat org.apache.hadoop.fs.ChecksumFileSystem.create(ChecksumFileSystem.java:458)\n",
      "\tat org.apache.hadoop.fs.ChecksumFileSystem.create(ChecksumFileSystem.java:443)\n",
      "\tat org.apache.hadoop.fs.FileSystem.create(FileSystem.java:1118)\n",
      "\tat org.apache.hadoop.fs.FileSystem.create(FileSystem.java:1098)\n",
      "\tat org.apache.hadoop.fs.FileSystem.create(FileSystem.java:987)\n",
      "\tat org.apache.spark.sql.execution.datasources.CodecStreams$.createOutputStream(CodecStreams.scala:81)\n",
      "\tat org.apache.spark.sql.execution.datasources.CodecStreams$.createOutputStreamWriter(CodecStreams.scala:92)\n",
      "\tat org.apache.spark.sql.execution.datasources.csv.CsvOutputWriter.<init>(CsvOutputWriter.scala:38)\n",
      "\tat org.apache.spark.sql.execution.datasources.csv.CSVFileFormat$$anon$1.newInstance(CSVFileFormat.scala:84)\n",
      "\tat org.apache.spark.sql.execution.datasources.SingleDirectoryDataWriter.newOutputWriter(FileFormatDataWriter.scala:126)\n",
      "\tat org.apache.spark.sql.execution.datasources.SingleDirectoryDataWriter.<init>(FileFormatDataWriter.scala:111)\n",
      "\tat org.apache.spark.sql.execution.datasources.FileFormatWriter$.executeTask(FileFormatWriter.scala:269)\n",
      "\tat org.apache.spark.sql.execution.datasources.FileFormatWriter$.$anonfun$write$15(FileFormatWriter.scala:210)\n",
      "\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:90)\n",
      "\tat org.apache.spark.scheduler.Task.run(Task.scala:131)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$3(Executor.scala:497)\n",
      "\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1439)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:500)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n",
      "\tat java.lang.Thread.run(Thread.java:748)\n",
      "\n",
      "21/09/09 11:18:56 WARN TaskSetManager: Lost task 169.2 in stage 51.0 (TID 2263) (192.168.1.2 executor 0): java.io.IOException: Mkdirs failed to create file:/home/geno1664/Developments/Github_Samples/RDS-ENV/Rural_Development_Study_No2/IO/output.csv/_temporary/0/_temporary/attempt_202109091118555914438970598811046_0051_m_000169_2263 (exists=false, cwd=file:/home/hadoop/.workspace/app-20210909111836-0003/0)\n",
      "\tat org.apache.hadoop.fs.ChecksumFileSystem.create(ChecksumFileSystem.java:458)\n",
      "\tat org.apache.hadoop.fs.ChecksumFileSystem.create(ChecksumFileSystem.java:443)\n",
      "\tat org.apache.hadoop.fs.FileSystem.create(FileSystem.java:1118)\n",
      "\tat org.apache.hadoop.fs.FileSystem.create(FileSystem.java:1098)\n",
      "\tat org.apache.hadoop.fs.FileSystem.create(FileSystem.java:987)\n",
      "\tat org.apache.spark.sql.execution.datasources.CodecStreams$.createOutputStream(CodecStreams.scala:81)\n",
      "\tat org.apache.spark.sql.execution.datasources.CodecStreams$.createOutputStreamWriter(CodecStreams.scala:92)\n",
      "\tat org.apache.spark.sql.execution.datasources.csv.CsvOutputWriter.<init>(CsvOutputWriter.scala:38)\n",
      "\tat org.apache.spark.sql.execution.datasources.csv.CSVFileFormat$$anon$1.newInstance(CSVFileFormat.scala:84)\n",
      "\tat org.apache.spark.sql.execution.datasources.SingleDirectoryDataWriter.newOutputWriter(FileFormatDataWriter.scala:126)\n",
      "\tat org.apache.spark.sql.execution.datasources.SingleDirectoryDataWriter.<init>(FileFormatDataWriter.scala:111)\n",
      "\tat org.apache.spark.sql.execution.datasources.FileFormatWriter$.executeTask(FileFormatWriter.scala:269)\n",
      "\tat org.apache.spark.sql.execution.datasources.FileFormatWriter$.$anonfun$write$15(FileFormatWriter.scala:210)\n",
      "\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:90)\n",
      "\tat org.apache.spark.scheduler.Task.run(Task.scala:131)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$3(Executor.scala:497)\n",
      "\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1439)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:500)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n",
      "\tat java.lang.Thread.run(Thread.java:748)\n",
      "\n",
      "21/09/09 11:18:56 WARN TaskSetManager: Lost task 103.2 in stage 51.0 (TID 2262) (192.168.1.2 executor 0): java.io.IOException: Mkdirs failed to create file:/home/geno1664/Developments/Github_Samples/RDS-ENV/Rural_Development_Study_No2/IO/output.csv/_temporary/0/_temporary/attempt_202109091118557773088834050265639_0051_m_000103_2262 (exists=false, cwd=file:/home/hadoop/.workspace/app-20210909111836-0003/0)\n",
      "\tat org.apache.hadoop.fs.ChecksumFileSystem.create(ChecksumFileSystem.java:458)\n",
      "\tat org.apache.hadoop.fs.ChecksumFileSystem.create(ChecksumFileSystem.java:443)\n",
      "\tat org.apache.hadoop.fs.FileSystem.create(FileSystem.java:1118)\n",
      "\tat org.apache.hadoop.fs.FileSystem.create(FileSystem.java:1098)\n",
      "\tat org.apache.hadoop.fs.FileSystem.create(FileSystem.java:987)\n",
      "\tat org.apache.spark.sql.execution.datasources.CodecStreams$.createOutputStream(CodecStreams.scala:81)\n",
      "\tat org.apache.spark.sql.execution.datasources.CodecStreams$.createOutputStreamWriter(CodecStreams.scala:92)\n",
      "\tat org.apache.spark.sql.execution.datasources.csv.CsvOutputWriter.<init>(CsvOutputWriter.scala:38)\n",
      "\tat org.apache.spark.sql.execution.datasources.csv.CSVFileFormat$$anon$1.newInstance(CSVFileFormat.scala:84)\n",
      "\tat org.apache.spark.sql.execution.datasources.SingleDirectoryDataWriter.newOutputWriter(FileFormatDataWriter.scala:126)\n",
      "\tat org.apache.spark.sql.execution.datasources.SingleDirectoryDataWriter.<init>(FileFormatDataWriter.scala:111)\n",
      "\tat org.apache.spark.sql.execution.datasources.FileFormatWriter$.executeTask(FileFormatWriter.scala:269)\n",
      "\tat org.apache.spark.sql.execution.datasources.FileFormatWriter$.$anonfun$write$15(FileFormatWriter.scala:210)\n",
      "\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:90)\n",
      "\tat org.apache.spark.scheduler.Task.run(Task.scala:131)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$3(Executor.scala:497)\n",
      "\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1439)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:500)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n",
      "\tat java.lang.Thread.run(Thread.java:748)\n",
      "\n",
      "21/09/09 11:18:56 WARN TaskSetManager: Lost task 33.3 in stage 51.0 (TID 2265) (192.168.1.2 executor 0): java.io.IOException: Mkdirs failed to create file:/home/geno1664/Developments/Github_Samples/RDS-ENV/Rural_Development_Study_No2/IO/output.csv/_temporary/0/_temporary/attempt_202109091118556258811576982811410_0051_m_000033_2265 (exists=false, cwd=file:/home/hadoop/.workspace/app-20210909111836-0003/0)\n",
      "\tat org.apache.hadoop.fs.ChecksumFileSystem.create(ChecksumFileSystem.java:458)\n",
      "\tat org.apache.hadoop.fs.ChecksumFileSystem.create(ChecksumFileSystem.java:443)\n",
      "\tat org.apache.hadoop.fs.FileSystem.create(FileSystem.java:1118)\n",
      "\tat org.apache.hadoop.fs.FileSystem.create(FileSystem.java:1098)\n",
      "\tat org.apache.hadoop.fs.FileSystem.create(FileSystem.java:987)\n",
      "\tat org.apache.spark.sql.execution.datasources.CodecStreams$.createOutputStream(CodecStreams.scala:81)\n",
      "\tat org.apache.spark.sql.execution.datasources.CodecStreams$.createOutputStreamWriter(CodecStreams.scala:92)\n",
      "\tat org.apache.spark.sql.execution.datasources.csv.CsvOutputWriter.<init>(CsvOutputWriter.scala:38)\n",
      "\tat org.apache.spark.sql.execution.datasources.csv.CSVFileFormat$$anon$1.newInstance(CSVFileFormat.scala:84)\n",
      "\tat org.apache.spark.sql.execution.datasources.SingleDirectoryDataWriter.newOutputWriter(FileFormatDataWriter.scala:126)\n",
      "\tat org.apache.spark.sql.execution.datasources.SingleDirectoryDataWriter.<init>(FileFormatDataWriter.scala:111)\n",
      "\tat org.apache.spark.sql.execution.datasources.FileFormatWriter$.executeTask(FileFormatWriter.scala:269)\n",
      "\tat org.apache.spark.sql.execution.datasources.FileFormatWriter$.$anonfun$write$15(FileFormatWriter.scala:210)\n",
      "\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:90)\n",
      "\tat org.apache.spark.scheduler.Task.run(Task.scala:131)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$3(Executor.scala:497)\n",
      "\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1439)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:500)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n",
      "\tat java.lang.Thread.run(Thread.java:748)\n",
      "\n",
      "21/09/09 11:18:56 ERROR TaskSetManager: Task 33 in stage 51.0 failed 4 times; aborting job\n",
      "21/09/09 11:18:56 WARN TaskSetManager: Lost task 8.2 in stage 51.0 (TID 2259) (192.168.1.2 executor 0): java.io.IOException: Mkdirs failed to create file:/home/geno1664/Developments/Github_Samples/RDS-ENV/Rural_Development_Study_No2/IO/output.csv/_temporary/0/_temporary/attempt_202109091118558189771698519006176_0051_m_000008_2259 (exists=false, cwd=file:/home/hadoop/.workspace/app-20210909111836-0003/0)\n",
      "\tat org.apache.hadoop.fs.ChecksumFileSystem.create(ChecksumFileSystem.java:458)\n",
      "\tat org.apache.hadoop.fs.ChecksumFileSystem.create(ChecksumFileSystem.java:443)\n",
      "\tat org.apache.hadoop.fs.FileSystem.create(FileSystem.java:1118)\n",
      "\tat org.apache.hadoop.fs.FileSystem.create(FileSystem.java:1098)\n",
      "\tat org.apache.hadoop.fs.FileSystem.create(FileSystem.java:987)\n",
      "\tat org.apache.spark.sql.execution.datasources.CodecStreams$.createOutputStream(CodecStreams.scala:81)\n",
      "\tat org.apache.spark.sql.execution.datasources.CodecStreams$.createOutputStreamWriter(CodecStreams.scala:92)\n",
      "\tat org.apache.spark.sql.execution.datasources.csv.CsvOutputWriter.<init>(CsvOutputWriter.scala:38)\n",
      "\tat org.apache.spark.sql.execution.datasources.csv.CSVFileFormat$$anon$1.newInstance(CSVFileFormat.scala:84)\n",
      "\tat org.apache.spark.sql.execution.datasources.SingleDirectoryDataWriter.newOutputWriter(FileFormatDataWriter.scala:126)\n",
      "\tat org.apache.spark.sql.execution.datasources.SingleDirectoryDataWriter.<init>(FileFormatDataWriter.scala:111)\n",
      "\tat org.apache.spark.sql.execution.datasources.FileFormatWriter$.executeTask(FileFormatWriter.scala:269)\n",
      "\tat org.apache.spark.sql.execution.datasources.FileFormatWriter$.$anonfun$write$15(FileFormatWriter.scala:210)\n",
      "\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:90)\n",
      "\tat org.apache.spark.scheduler.Task.run(Task.scala:131)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$3(Executor.scala:497)\n",
      "\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1439)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:500)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n",
      "\tat java.lang.Thread.run(Thread.java:748)\n",
      "\n",
      "21/09/09 11:18:56 WARN TaskSetManager: Lost task 59.2 in stage 51.0 (TID 2264) (192.168.1.2 executor 0): java.io.IOException: Mkdirs failed to create file:/home/geno1664/Developments/Github_Samples/RDS-ENV/Rural_Development_Study_No2/IO/output.csv/_temporary/0/_temporary/attempt_202109091118557135842122639550627_0051_m_000059_2264 (exists=false, cwd=file:/home/hadoop/.workspace/app-20210909111836-0003/0)\n",
      "\tat org.apache.hadoop.fs.ChecksumFileSystem.create(ChecksumFileSystem.java:458)\n",
      "\tat org.apache.hadoop.fs.ChecksumFileSystem.create(ChecksumFileSystem.java:443)\n",
      "\tat org.apache.hadoop.fs.FileSystem.create(FileSystem.java:1118)\n",
      "\tat org.apache.hadoop.fs.FileSystem.create(FileSystem.java:1098)\n",
      "\tat org.apache.hadoop.fs.FileSystem.create(FileSystem.java:987)\n",
      "\tat org.apache.spark.sql.execution.datasources.CodecStreams$.createOutputStream(CodecStreams.scala:81)\n",
      "\tat org.apache.spark.sql.execution.datasources.CodecStreams$.createOutputStreamWriter(CodecStreams.scala:92)\n",
      "\tat org.apache.spark.sql.execution.datasources.csv.CsvOutputWriter.<init>(CsvOutputWriter.scala:38)\n",
      "\tat org.apache.spark.sql.execution.datasources.csv.CSVFileFormat$$anon$1.newInstance(CSVFileFormat.scala:84)\n",
      "\tat org.apache.spark.sql.execution.datasources.SingleDirectoryDataWriter.newOutputWriter(FileFormatDataWriter.scala:126)\n",
      "\tat org.apache.spark.sql.execution.datasources.SingleDirectoryDataWriter.<init>(FileFormatDataWriter.scala:111)\n",
      "\tat org.apache.spark.sql.execution.datasources.FileFormatWriter$.executeTask(FileFormatWriter.scala:269)\n",
      "\tat org.apache.spark.sql.execution.datasources.FileFormatWriter$.$anonfun$write$15(FileFormatWriter.scala:210)\n",
      "\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:90)\n",
      "\tat org.apache.spark.scheduler.Task.run(Task.scala:131)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$3(Executor.scala:497)\n",
      "\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1439)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:500)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n",
      "\tat java.lang.Thread.run(Thread.java:748)\n",
      "\n",
      "21/09/09 11:18:56 WARN TaskSetManager: Lost task 130.3 in stage 51.0 (TID 2266) (192.168.1.2 executor 0): java.io.IOException: Mkdirs failed to create file:/home/geno1664/Developments/Github_Samples/RDS-ENV/Rural_Development_Study_No2/IO/output.csv/_temporary/0/_temporary/attempt_202109091118555425329606843925781_0051_m_000130_2266 (exists=false, cwd=file:/home/hadoop/.workspace/app-20210909111836-0003/0)\n",
      "\tat org.apache.hadoop.fs.ChecksumFileSystem.create(ChecksumFileSystem.java:458)\n",
      "\tat org.apache.hadoop.fs.ChecksumFileSystem.create(ChecksumFileSystem.java:443)\n",
      "\tat org.apache.hadoop.fs.FileSystem.create(FileSystem.java:1118)\n",
      "\tat org.apache.hadoop.fs.FileSystem.create(FileSystem.java:1098)\n",
      "\tat org.apache.hadoop.fs.FileSystem.create(FileSystem.java:987)\n",
      "\tat org.apache.spark.sql.execution.datasources.CodecStreams$.createOutputStream(CodecStreams.scala:81)\n",
      "\tat org.apache.spark.sql.execution.datasources.CodecStreams$.createOutputStreamWriter(CodecStreams.scala:92)\n",
      "\tat org.apache.spark.sql.execution.datasources.csv.CsvOutputWriter.<init>(CsvOutputWriter.scala:38)\n",
      "\tat org.apache.spark.sql.execution.datasources.csv.CSVFileFormat$$anon$1.newInstance(CSVFileFormat.scala:84)\n",
      "\tat org.apache.spark.sql.execution.datasources.SingleDirectoryDataWriter.newOutputWriter(FileFormatDataWriter.scala:126)\n",
      "\tat org.apache.spark.sql.execution.datasources.SingleDirectoryDataWriter.<init>(FileFormatDataWriter.scala:111)\n",
      "\tat org.apache.spark.sql.execution.datasources.FileFormatWriter$.executeTask(FileFormatWriter.scala:269)\n",
      "\tat org.apache.spark.sql.execution.datasources.FileFormatWriter$.$anonfun$write$15(FileFormatWriter.scala:210)\n",
      "\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:90)\n",
      "\tat org.apache.spark.scheduler.Task.run(Task.scala:131)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$3(Executor.scala:497)\n",
      "\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1439)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:500)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n",
      "\tat java.lang.Thread.run(Thread.java:748)\n",
      "\n",
      "21/09/09 11:18:56 ERROR FileFormatWriter: Aborting job 886e072a-f455-40f4-bb5d-4ff5148a6a3a.\n",
      "org.apache.spark.SparkException: Job aborted due to stage failure: Task 33 in stage 51.0 failed 4 times, most recent failure: Lost task 33.3 in stage 51.0 (TID 2265) (192.168.1.2 executor 0): java.io.IOException: Mkdirs failed to create file:/home/geno1664/Developments/Github_Samples/RDS-ENV/Rural_Development_Study_No2/IO/output.csv/_temporary/0/_temporary/attempt_202109091118556258811576982811410_0051_m_000033_2265 (exists=false, cwd=file:/home/hadoop/.workspace/app-20210909111836-0003/0)\n",
      "\tat org.apache.hadoop.fs.ChecksumFileSystem.create(ChecksumFileSystem.java:458)\n",
      "\tat org.apache.hadoop.fs.ChecksumFileSystem.create(ChecksumFileSystem.java:443)\n",
      "\tat org.apache.hadoop.fs.FileSystem.create(FileSystem.java:1118)\n",
      "\tat org.apache.hadoop.fs.FileSystem.create(FileSystem.java:1098)\n",
      "\tat org.apache.hadoop.fs.FileSystem.create(FileSystem.java:987)\n",
      "\tat org.apache.spark.sql.execution.datasources.CodecStreams$.createOutputStream(CodecStreams.scala:81)\n",
      "\tat org.apache.spark.sql.execution.datasources.CodecStreams$.createOutputStreamWriter(CodecStreams.scala:92)\n",
      "\tat org.apache.spark.sql.execution.datasources.csv.CsvOutputWriter.<init>(CsvOutputWriter.scala:38)\n",
      "\tat org.apache.spark.sql.execution.datasources.csv.CSVFileFormat$$anon$1.newInstance(CSVFileFormat.scala:84)\n",
      "\tat org.apache.spark.sql.execution.datasources.SingleDirectoryDataWriter.newOutputWriter(FileFormatDataWriter.scala:126)\n",
      "\tat org.apache.spark.sql.execution.datasources.SingleDirectoryDataWriter.<init>(FileFormatDataWriter.scala:111)\n",
      "\tat org.apache.spark.sql.execution.datasources.FileFormatWriter$.executeTask(FileFormatWriter.scala:269)\n",
      "\tat org.apache.spark.sql.execution.datasources.FileFormatWriter$.$anonfun$write$15(FileFormatWriter.scala:210)\n",
      "\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:90)\n",
      "\tat org.apache.spark.scheduler.Task.run(Task.scala:131)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$3(Executor.scala:497)\n",
      "\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1439)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:500)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n",
      "\tat java.lang.Thread.run(Thread.java:748)\n",
      "\n",
      "Driver stacktrace:\n",
      "\tat org.apache.spark.scheduler.DAGScheduler.failJobAndIndependentStages(DAGScheduler.scala:2258)\n",
      "\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2(DAGScheduler.scala:2207)\n",
      "\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2$adapted(DAGScheduler.scala:2206)\n",
      "\tat scala.collection.mutable.ResizableArray.foreach(ResizableArray.scala:62)\n",
      "\tat scala.collection.mutable.ResizableArray.foreach$(ResizableArray.scala:55)\n",
      "\tat scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:49)\n",
      "\tat org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:2206)\n",
      "\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1(DAGScheduler.scala:1079)\n",
      "\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1$adapted(DAGScheduler.scala:1079)\n",
      "\tat scala.Option.foreach(Option.scala:407)\n",
      "\tat org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:1079)\n",
      "\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:2445)\n",
      "\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2387)\n",
      "\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2376)\n",
      "\tat org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:49)\n",
      "\tat org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:868)\n",
      "\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2196)\n",
      "\tat org.apache.spark.sql.execution.datasources.FileFormatWriter$.write(FileFormatWriter.scala:200)\n",
      "\tat org.apache.spark.sql.execution.datasources.InsertIntoHadoopFsRelationCommand.run(InsertIntoHadoopFsRelationCommand.scala:188)\n",
      "\tat org.apache.spark.sql.execution.command.DataWritingCommandExec.sideEffectResult$lzycompute(commands.scala:108)\n",
      "\tat org.apache.spark.sql.execution.command.DataWritingCommandExec.sideEffectResult(commands.scala:106)\n",
      "\tat org.apache.spark.sql.execution.command.DataWritingCommandExec.doExecute(commands.scala:131)\n",
      "\tat org.apache.spark.sql.execution.SparkPlan.$anonfun$execute$1(SparkPlan.scala:180)\n",
      "\tat org.apache.spark.sql.execution.SparkPlan.$anonfun$executeQuery$1(SparkPlan.scala:218)\n",
      "\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)\n",
      "\tat org.apache.spark.sql.execution.SparkPlan.executeQuery(SparkPlan.scala:215)\n",
      "\tat org.apache.spark.sql.execution.SparkPlan.execute(SparkPlan.scala:176)\n",
      "\tat org.apache.spark.sql.execution.QueryExecution.toRdd$lzycompute(QueryExecution.scala:132)\n",
      "\tat org.apache.spark.sql.execution.QueryExecution.toRdd(QueryExecution.scala:131)\n",
      "\tat org.apache.spark.sql.DataFrameWriter.$anonfun$runCommand$1(DataFrameWriter.scala:989)\n",
      "\tat org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$5(SQLExecution.scala:103)\n",
      "\tat org.apache.spark.sql.execution.SQLExecution$.withSQLConfPropagated(SQLExecution.scala:163)\n",
      "\tat org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$1(SQLExecution.scala:90)\n",
      "\tat org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:775)\n",
      "\tat org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId(SQLExecution.scala:64)\n",
      "\tat org.apache.spark.sql.DataFrameWriter.runCommand(DataFrameWriter.scala:989)\n",
      "\tat org.apache.spark.sql.DataFrameWriter.saveToV1Source(DataFrameWriter.scala:438)\n",
      "\tat org.apache.spark.sql.DataFrameWriter.saveInternal(DataFrameWriter.scala:415)\n",
      "\tat org.apache.spark.sql.DataFrameWriter.save(DataFrameWriter.scala:293)\n",
      "\tat sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n",
      "\tat sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\n",
      "\tat sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n",
      "\tat java.lang.reflect.Method.invoke(Method.java:498)\n",
      "\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\n",
      "\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357)\n",
      "\tat py4j.Gateway.invoke(Gateway.java:282)\n",
      "\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\n",
      "\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\n",
      "\tat py4j.GatewayConnection.run(GatewayConnection.java:238)\n",
      "\tat java.lang.Thread.run(Thread.java:748)\n",
      "Caused by: java.io.IOException: Mkdirs failed to create file:/home/geno1664/Developments/Github_Samples/RDS-ENV/Rural_Development_Study_No2/IO/output.csv/_temporary/0/_temporary/attempt_202109091118556258811576982811410_0051_m_000033_2265 (exists=false, cwd=file:/home/hadoop/.workspace/app-20210909111836-0003/0)\n",
      "\tat org.apache.hadoop.fs.ChecksumFileSystem.create(ChecksumFileSystem.java:458)\n",
      "\tat org.apache.hadoop.fs.ChecksumFileSystem.create(ChecksumFileSystem.java:443)\n",
      "\tat org.apache.hadoop.fs.FileSystem.create(FileSystem.java:1118)\n",
      "\tat org.apache.hadoop.fs.FileSystem.create(FileSystem.java:1098)\n",
      "\tat org.apache.hadoop.fs.FileSystem.create(FileSystem.java:987)\n",
      "\tat org.apache.spark.sql.execution.datasources.CodecStreams$.createOutputStream(CodecStreams.scala:81)\n",
      "\tat org.apache.spark.sql.execution.datasources.CodecStreams$.createOutputStreamWriter(CodecStreams.scala:92)\n",
      "\tat org.apache.spark.sql.execution.datasources.csv.CsvOutputWriter.<init>(CsvOutputWriter.scala:38)\n",
      "\tat org.apache.spark.sql.execution.datasources.csv.CSVFileFormat$$anon$1.newInstance(CSVFileFormat.scala:84)\n",
      "\tat org.apache.spark.sql.execution.datasources.SingleDirectoryDataWriter.newOutputWriter(FileFormatDataWriter.scala:126)\n",
      "\tat org.apache.spark.sql.execution.datasources.SingleDirectoryDataWriter.<init>(FileFormatDataWriter.scala:111)\n",
      "\tat org.apache.spark.sql.execution.datasources.FileFormatWriter$.executeTask(FileFormatWriter.scala:269)\n",
      "\tat org.apache.spark.sql.execution.datasources.FileFormatWriter$.$anonfun$write$15(FileFormatWriter.scala:210)\n",
      "\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:90)\n",
      "\tat org.apache.spark.scheduler.Task.run(Task.scala:131)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$3(Executor.scala:497)\n",
      "\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1439)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:500)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n",
      "\t... 1 more\n",
      "21/09/09 11:18:56 WARN TaskSetManager: Lost task 0.2 in stage 51.0 (TID 2270) (192.168.1.2 executor 0): java.io.IOException: Mkdirs failed to create file:/home/geno1664/Developments/Github_Samples/RDS-ENV/Rural_Development_Study_No2/IO/output.csv/_temporary/0/_temporary/attempt_202109091118558167965381715562462_0051_m_000000_2270 (exists=false, cwd=file:/home/hadoop/.workspace/app-20210909111836-0003/0)\n",
      "\tat org.apache.hadoop.fs.ChecksumFileSystem.create(ChecksumFileSystem.java:458)\n",
      "\tat org.apache.hadoop.fs.ChecksumFileSystem.create(ChecksumFileSystem.java:443)\n",
      "\tat org.apache.hadoop.fs.FileSystem.create(FileSystem.java:1118)\n",
      "\tat org.apache.hadoop.fs.FileSystem.create(FileSystem.java:1098)\n",
      "\tat org.apache.hadoop.fs.FileSystem.create(FileSystem.java:987)\n",
      "\tat org.apache.spark.sql.execution.datasources.CodecStreams$.createOutputStream(CodecStreams.scala:81)\n",
      "\tat org.apache.spark.sql.execution.datasources.CodecStreams$.createOutputStreamWriter(CodecStreams.scala:92)\n",
      "\tat org.apache.spark.sql.execution.datasources.csv.CsvOutputWriter.<init>(CsvOutputWriter.scala:38)\n",
      "\tat org.apache.spark.sql.execution.datasources.csv.CSVFileFormat$$anon$1.newInstance(CSVFileFormat.scala:84)\n",
      "\tat org.apache.spark.sql.execution.datasources.SingleDirectoryDataWriter.newOutputWriter(FileFormatDataWriter.scala:126)\n",
      "\tat org.apache.spark.sql.execution.datasources.SingleDirectoryDataWriter.<init>(FileFormatDataWriter.scala:111)\n",
      "\tat org.apache.spark.sql.execution.datasources.FileFormatWriter$.executeTask(FileFormatWriter.scala:269)\n",
      "\tat org.apache.spark.sql.execution.datasources.FileFormatWriter$.$anonfun$write$15(FileFormatWriter.scala:210)\n",
      "\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:90)\n",
      "\tat org.apache.spark.scheduler.Task.run(Task.scala:131)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$3(Executor.scala:497)\n",
      "\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1439)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:500)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n",
      "\tat java.lang.Thread.run(Thread.java:748)\n",
      "\n",
      "21/09/09 11:18:57 WARN TaskSetManager: Lost task 79.2 in stage 51.0 (TID 2268) (192.168.1.2 executor 0): java.io.IOException: Mkdirs failed to create file:/home/geno1664/Developments/Github_Samples/RDS-ENV/Rural_Development_Study_No2/IO/output.csv/_temporary/0/_temporary/attempt_202109091118556464308933655286932_0051_m_000079_2268 (exists=false, cwd=file:/home/hadoop/.workspace/app-20210909111836-0003/0)\n",
      "\tat org.apache.hadoop.fs.ChecksumFileSystem.create(ChecksumFileSystem.java:458)\n",
      "\tat org.apache.hadoop.fs.ChecksumFileSystem.create(ChecksumFileSystem.java:443)\n",
      "\tat org.apache.hadoop.fs.FileSystem.create(FileSystem.java:1118)\n",
      "\tat org.apache.hadoop.fs.FileSystem.create(FileSystem.java:1098)\n",
      "\tat org.apache.hadoop.fs.FileSystem.create(FileSystem.java:987)\n",
      "\tat org.apache.spark.sql.execution.datasources.CodecStreams$.createOutputStream(CodecStreams.scala:81)\n",
      "\tat org.apache.spark.sql.execution.datasources.CodecStreams$.createOutputStreamWriter(CodecStreams.scala:92)\n",
      "\tat org.apache.spark.sql.execution.datasources.csv.CsvOutputWriter.<init>(CsvOutputWriter.scala:38)\n",
      "\tat org.apache.spark.sql.execution.datasources.csv.CSVFileFormat$$anon$1.newInstance(CSVFileFormat.scala:84)\n",
      "\tat org.apache.spark.sql.execution.datasources.SingleDirectoryDataWriter.newOutputWriter(FileFormatDataWriter.scala:126)\n",
      "\tat org.apache.spark.sql.execution.datasources.SingleDirectoryDataWriter.<init>(FileFormatDataWriter.scala:111)\n",
      "\tat org.apache.spark.sql.execution.datasources.FileFormatWriter$.executeTask(FileFormatWriter.scala:269)\n",
      "\tat org.apache.spark.sql.execution.datasources.FileFormatWriter$.$anonfun$write$15(FileFormatWriter.scala:210)\n",
      "\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:90)\n",
      "\tat org.apache.spark.scheduler.Task.run(Task.scala:131)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$3(Executor.scala:497)\n",
      "\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1439)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:500)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n",
      "\tat java.lang.Thread.run(Thread.java:748)\n",
      "\n",
      "21/09/09 11:18:57 WARN TaskSetManager: Lost task 11.2 in stage 51.0 (TID 2269) (192.168.1.2 executor 0): TaskKilled (Stage cancelled)\n",
      "21/09/09 11:18:57 WARN TaskSetManager: Lost task 10.0 in stage 51.0 (TID 2271) (192.168.1.2 executor 0): TaskKilled (Stage cancelled)\n",
      "21/09/09 11:18:57 WARN TaskSetManager: Lost task 22.3 in stage 51.0 (TID 2267) (192.168.1.2 executor 0): TaskKilled (Stage cancelled)\n"
     ]
    },
    {
     "output_type": "error",
     "ename": "Py4JJavaError",
     "evalue": "An error occurred while calling o882.save.\n: org.apache.spark.SparkException: Job aborted.\n\tat org.apache.spark.sql.execution.datasources.FileFormatWriter$.write(FileFormatWriter.scala:231)\n\tat org.apache.spark.sql.execution.datasources.InsertIntoHadoopFsRelationCommand.run(InsertIntoHadoopFsRelationCommand.scala:188)\n\tat org.apache.spark.sql.execution.command.DataWritingCommandExec.sideEffectResult$lzycompute(commands.scala:108)\n\tat org.apache.spark.sql.execution.command.DataWritingCommandExec.sideEffectResult(commands.scala:106)\n\tat org.apache.spark.sql.execution.command.DataWritingCommandExec.doExecute(commands.scala:131)\n\tat org.apache.spark.sql.execution.SparkPlan.$anonfun$execute$1(SparkPlan.scala:180)\n\tat org.apache.spark.sql.execution.SparkPlan.$anonfun$executeQuery$1(SparkPlan.scala:218)\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)\n\tat org.apache.spark.sql.execution.SparkPlan.executeQuery(SparkPlan.scala:215)\n\tat org.apache.spark.sql.execution.SparkPlan.execute(SparkPlan.scala:176)\n\tat org.apache.spark.sql.execution.QueryExecution.toRdd$lzycompute(QueryExecution.scala:132)\n\tat org.apache.spark.sql.execution.QueryExecution.toRdd(QueryExecution.scala:131)\n\tat org.apache.spark.sql.DataFrameWriter.$anonfun$runCommand$1(DataFrameWriter.scala:989)\n\tat org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$5(SQLExecution.scala:103)\n\tat org.apache.spark.sql.execution.SQLExecution$.withSQLConfPropagated(SQLExecution.scala:163)\n\tat org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$1(SQLExecution.scala:90)\n\tat org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:775)\n\tat org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId(SQLExecution.scala:64)\n\tat org.apache.spark.sql.DataFrameWriter.runCommand(DataFrameWriter.scala:989)\n\tat org.apache.spark.sql.DataFrameWriter.saveToV1Source(DataFrameWriter.scala:438)\n\tat org.apache.spark.sql.DataFrameWriter.saveInternal(DataFrameWriter.scala:415)\n\tat org.apache.spark.sql.DataFrameWriter.save(DataFrameWriter.scala:293)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\n\tat sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n\tat java.lang.reflect.Method.invoke(Method.java:498)\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357)\n\tat py4j.Gateway.invoke(Gateway.java:282)\n\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\n\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\n\tat py4j.GatewayConnection.run(GatewayConnection.java:238)\n\tat java.lang.Thread.run(Thread.java:748)\nCaused by: org.apache.spark.SparkException: Job aborted due to stage failure: Task 33 in stage 51.0 failed 4 times, most recent failure: Lost task 33.3 in stage 51.0 (TID 2265) (192.168.1.2 executor 0): java.io.IOException: Mkdirs failed to create file:/home/geno1664/Developments/Github_Samples/RDS-ENV/Rural_Development_Study_No2/IO/output.csv/_temporary/0/_temporary/attempt_202109091118556258811576982811410_0051_m_000033_2265 (exists=false, cwd=file:/home/hadoop/.workspace/app-20210909111836-0003/0)\n\tat org.apache.hadoop.fs.ChecksumFileSystem.create(ChecksumFileSystem.java:458)\n\tat org.apache.hadoop.fs.ChecksumFileSystem.create(ChecksumFileSystem.java:443)\n\tat org.apache.hadoop.fs.FileSystem.create(FileSystem.java:1118)\n\tat org.apache.hadoop.fs.FileSystem.create(FileSystem.java:1098)\n\tat org.apache.hadoop.fs.FileSystem.create(FileSystem.java:987)\n\tat org.apache.spark.sql.execution.datasources.CodecStreams$.createOutputStream(CodecStreams.scala:81)\n\tat org.apache.spark.sql.execution.datasources.CodecStreams$.createOutputStreamWriter(CodecStreams.scala:92)\n\tat org.apache.spark.sql.execution.datasources.csv.CsvOutputWriter.<init>(CsvOutputWriter.scala:38)\n\tat org.apache.spark.sql.execution.datasources.csv.CSVFileFormat$$anon$1.newInstance(CSVFileFormat.scala:84)\n\tat org.apache.spark.sql.execution.datasources.SingleDirectoryDataWriter.newOutputWriter(FileFormatDataWriter.scala:126)\n\tat org.apache.spark.sql.execution.datasources.SingleDirectoryDataWriter.<init>(FileFormatDataWriter.scala:111)\n\tat org.apache.spark.sql.execution.datasources.FileFormatWriter$.executeTask(FileFormatWriter.scala:269)\n\tat org.apache.spark.sql.execution.datasources.FileFormatWriter$.$anonfun$write$15(FileFormatWriter.scala:210)\n\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:90)\n\tat org.apache.spark.scheduler.Task.run(Task.scala:131)\n\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$3(Executor.scala:497)\n\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1439)\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:500)\n\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n\tat java.lang.Thread.run(Thread.java:748)\n\nDriver stacktrace:\n\tat org.apache.spark.scheduler.DAGScheduler.failJobAndIndependentStages(DAGScheduler.scala:2258)\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2(DAGScheduler.scala:2207)\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2$adapted(DAGScheduler.scala:2206)\n\tat scala.collection.mutable.ResizableArray.foreach(ResizableArray.scala:62)\n\tat scala.collection.mutable.ResizableArray.foreach$(ResizableArray.scala:55)\n\tat scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:49)\n\tat org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:2206)\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1(DAGScheduler.scala:1079)\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1$adapted(DAGScheduler.scala:1079)\n\tat scala.Option.foreach(Option.scala:407)\n\tat org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:1079)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:2445)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2387)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2376)\n\tat org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:49)\n\tat org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:868)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2196)\n\tat org.apache.spark.sql.execution.datasources.FileFormatWriter$.write(FileFormatWriter.scala:200)\n\t... 32 more\nCaused by: java.io.IOException: Mkdirs failed to create file:/home/geno1664/Developments/Github_Samples/RDS-ENV/Rural_Development_Study_No2/IO/output.csv/_temporary/0/_temporary/attempt_202109091118556258811576982811410_0051_m_000033_2265 (exists=false, cwd=file:/home/hadoop/.workspace/app-20210909111836-0003/0)\n\tat org.apache.hadoop.fs.ChecksumFileSystem.create(ChecksumFileSystem.java:458)\n\tat org.apache.hadoop.fs.ChecksumFileSystem.create(ChecksumFileSystem.java:443)\n\tat org.apache.hadoop.fs.FileSystem.create(FileSystem.java:1118)\n\tat org.apache.hadoop.fs.FileSystem.create(FileSystem.java:1098)\n\tat org.apache.hadoop.fs.FileSystem.create(FileSystem.java:987)\n\tat org.apache.spark.sql.execution.datasources.CodecStreams$.createOutputStream(CodecStreams.scala:81)\n\tat org.apache.spark.sql.execution.datasources.CodecStreams$.createOutputStreamWriter(CodecStreams.scala:92)\n\tat org.apache.spark.sql.execution.datasources.csv.CsvOutputWriter.<init>(CsvOutputWriter.scala:38)\n\tat org.apache.spark.sql.execution.datasources.csv.CSVFileFormat$$anon$1.newInstance(CSVFileFormat.scala:84)\n\tat org.apache.spark.sql.execution.datasources.SingleDirectoryDataWriter.newOutputWriter(FileFormatDataWriter.scala:126)\n\tat org.apache.spark.sql.execution.datasources.SingleDirectoryDataWriter.<init>(FileFormatDataWriter.scala:111)\n\tat org.apache.spark.sql.execution.datasources.FileFormatWriter$.executeTask(FileFormatWriter.scala:269)\n\tat org.apache.spark.sql.execution.datasources.FileFormatWriter$.$anonfun$write$15(FileFormatWriter.scala:210)\n\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:90)\n\tat org.apache.spark.scheduler.Task.run(Task.scala:131)\n\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$3(Executor.scala:497)\n\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1439)\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:500)\n\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n\t... 1 more\n",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mPy4JJavaError\u001b[0m                             Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_33843/3455165524.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mCSV_File\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mExportCSV\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'/home/geno1664/Developments/Github_Samples/RDS-ENV/Rural_Development_Study_No2/IO/output.csv'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmainDF\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m~/Developments/Github_Samples/RDS-ENV/Rural_Development_Study_No2/SRC/Functions/IO.py\u001b[0m in \u001b[0;36mExportCSV\u001b[0;34m(aPath, aDataframe, aColumns, aAppend, aIndexColumns)\u001b[0m\n\u001b[1;32m     24\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     25\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0maColumns\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 26\u001b[0;31m             \u001b[0maDataframe\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto_koalas\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto_csv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0maPath\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mna_rep\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'N/A'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdate_format\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'yyyy-mm-dd'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmode\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmode\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mindex_col\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0maIndexColumns\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     27\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     28\u001b[0m             \u001b[0maDataframe\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto_koalas\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto_csv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0maPath\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mna_rep\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'N/A'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdate_format\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'yyyy-mm-dd'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmode\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmode\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mindex_col\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0maIndexColumns\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcolumns\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0maColumns\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Developments/Github_Samples/RDS-ENV/lib/python3.8/site-packages/databricks/koalas/generic.py\u001b[0m in \u001b[0;36mto_csv\u001b[0;34m(self, path, sep, na_rep, columns, header, quotechar, date_format, escapechar, num_files, mode, partition_cols, index_col, **options)\u001b[0m\n\u001b[1;32m    836\u001b[0m             \u001b[0mcharToEscapeQuoteEscaping\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mescapechar\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    837\u001b[0m         )\n\u001b[0;32m--> 838\u001b[0;31m         \u001b[0mbuilder\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moptions\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m**\u001b[0m\u001b[0moptions\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"csv\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msave\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    839\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    840\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Developments/Github_Samples/RDS-ENV/lib/python3.8/site-packages/pyspark/sql/readwriter.py\u001b[0m in \u001b[0;36msave\u001b[0;34m(self, path, format, mode, partitionBy, **options)\u001b[0m\n\u001b[1;32m   1107\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jwrite\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msave\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1108\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1109\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jwrite\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msave\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1110\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1111\u001b[0m     \u001b[0;34m@\u001b[0m\u001b[0msince\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1.4\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Developments/Github_Samples/RDS-ENV/lib/python3.8/site-packages/py4j/java_gateway.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args)\u001b[0m\n\u001b[1;32m   1302\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1303\u001b[0m         \u001b[0manswer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgateway_client\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msend_command\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcommand\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1304\u001b[0;31m         return_value = get_return_value(\n\u001b[0m\u001b[1;32m   1305\u001b[0m             answer, self.gateway_client, self.target_id, self.name)\n\u001b[1;32m   1306\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Developments/Github_Samples/RDS-ENV/lib/python3.8/site-packages/pyspark/sql/utils.py\u001b[0m in \u001b[0;36mdeco\u001b[0;34m(*a, **kw)\u001b[0m\n\u001b[1;32m    109\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mdeco\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkw\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    110\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 111\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkw\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    112\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mpy4j\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mprotocol\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mPy4JJavaError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    113\u001b[0m             \u001b[0mconverted\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mconvert_exception\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjava_exception\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Developments/Github_Samples/RDS-ENV/lib/python3.8/site-packages/py4j/protocol.py\u001b[0m in \u001b[0;36mget_return_value\u001b[0;34m(answer, gateway_client, target_id, name)\u001b[0m\n\u001b[1;32m    324\u001b[0m             \u001b[0mvalue\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mOUTPUT_CONVERTER\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mtype\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0manswer\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgateway_client\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    325\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0manswer\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0mREFERENCE_TYPE\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 326\u001b[0;31m                 raise Py4JJavaError(\n\u001b[0m\u001b[1;32m    327\u001b[0m                     \u001b[0;34m\"An error occurred while calling {0}{1}{2}.\\n\"\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    328\u001b[0m                     format(target_id, \".\", name), value)\n",
      "\u001b[0;31mPy4JJavaError\u001b[0m: An error occurred while calling o882.save.\n: org.apache.spark.SparkException: Job aborted.\n\tat org.apache.spark.sql.execution.datasources.FileFormatWriter$.write(FileFormatWriter.scala:231)\n\tat org.apache.spark.sql.execution.datasources.InsertIntoHadoopFsRelationCommand.run(InsertIntoHadoopFsRelationCommand.scala:188)\n\tat org.apache.spark.sql.execution.command.DataWritingCommandExec.sideEffectResult$lzycompute(commands.scala:108)\n\tat org.apache.spark.sql.execution.command.DataWritingCommandExec.sideEffectResult(commands.scala:106)\n\tat org.apache.spark.sql.execution.command.DataWritingCommandExec.doExecute(commands.scala:131)\n\tat org.apache.spark.sql.execution.SparkPlan.$anonfun$execute$1(SparkPlan.scala:180)\n\tat org.apache.spark.sql.execution.SparkPlan.$anonfun$executeQuery$1(SparkPlan.scala:218)\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)\n\tat org.apache.spark.sql.execution.SparkPlan.executeQuery(SparkPlan.scala:215)\n\tat org.apache.spark.sql.execution.SparkPlan.execute(SparkPlan.scala:176)\n\tat org.apache.spark.sql.execution.QueryExecution.toRdd$lzycompute(QueryExecution.scala:132)\n\tat org.apache.spark.sql.execution.QueryExecution.toRdd(QueryExecution.scala:131)\n\tat org.apache.spark.sql.DataFrameWriter.$anonfun$runCommand$1(DataFrameWriter.scala:989)\n\tat org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$5(SQLExecution.scala:103)\n\tat org.apache.spark.sql.execution.SQLExecution$.withSQLConfPropagated(SQLExecution.scala:163)\n\tat org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$1(SQLExecution.scala:90)\n\tat org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:775)\n\tat org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId(SQLExecution.scala:64)\n\tat org.apache.spark.sql.DataFrameWriter.runCommand(DataFrameWriter.scala:989)\n\tat org.apache.spark.sql.DataFrameWriter.saveToV1Source(DataFrameWriter.scala:438)\n\tat org.apache.spark.sql.DataFrameWriter.saveInternal(DataFrameWriter.scala:415)\n\tat org.apache.spark.sql.DataFrameWriter.save(DataFrameWriter.scala:293)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\n\tat sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n\tat java.lang.reflect.Method.invoke(Method.java:498)\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357)\n\tat py4j.Gateway.invoke(Gateway.java:282)\n\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\n\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\n\tat py4j.GatewayConnection.run(GatewayConnection.java:238)\n\tat java.lang.Thread.run(Thread.java:748)\nCaused by: org.apache.spark.SparkException: Job aborted due to stage failure: Task 33 in stage 51.0 failed 4 times, most recent failure: Lost task 33.3 in stage 51.0 (TID 2265) (192.168.1.2 executor 0): java.io.IOException: Mkdirs failed to create file:/home/geno1664/Developments/Github_Samples/RDS-ENV/Rural_Development_Study_No2/IO/output.csv/_temporary/0/_temporary/attempt_202109091118556258811576982811410_0051_m_000033_2265 (exists=false, cwd=file:/home/hadoop/.workspace/app-20210909111836-0003/0)\n\tat org.apache.hadoop.fs.ChecksumFileSystem.create(ChecksumFileSystem.java:458)\n\tat org.apache.hadoop.fs.ChecksumFileSystem.create(ChecksumFileSystem.java:443)\n\tat org.apache.hadoop.fs.FileSystem.create(FileSystem.java:1118)\n\tat org.apache.hadoop.fs.FileSystem.create(FileSystem.java:1098)\n\tat org.apache.hadoop.fs.FileSystem.create(FileSystem.java:987)\n\tat org.apache.spark.sql.execution.datasources.CodecStreams$.createOutputStream(CodecStreams.scala:81)\n\tat org.apache.spark.sql.execution.datasources.CodecStreams$.createOutputStreamWriter(CodecStreams.scala:92)\n\tat org.apache.spark.sql.execution.datasources.csv.CsvOutputWriter.<init>(CsvOutputWriter.scala:38)\n\tat org.apache.spark.sql.execution.datasources.csv.CSVFileFormat$$anon$1.newInstance(CSVFileFormat.scala:84)\n\tat org.apache.spark.sql.execution.datasources.SingleDirectoryDataWriter.newOutputWriter(FileFormatDataWriter.scala:126)\n\tat org.apache.spark.sql.execution.datasources.SingleDirectoryDataWriter.<init>(FileFormatDataWriter.scala:111)\n\tat org.apache.spark.sql.execution.datasources.FileFormatWriter$.executeTask(FileFormatWriter.scala:269)\n\tat org.apache.spark.sql.execution.datasources.FileFormatWriter$.$anonfun$write$15(FileFormatWriter.scala:210)\n\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:90)\n\tat org.apache.spark.scheduler.Task.run(Task.scala:131)\n\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$3(Executor.scala:497)\n\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1439)\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:500)\n\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n\tat java.lang.Thread.run(Thread.java:748)\n\nDriver stacktrace:\n\tat org.apache.spark.scheduler.DAGScheduler.failJobAndIndependentStages(DAGScheduler.scala:2258)\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2(DAGScheduler.scala:2207)\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2$adapted(DAGScheduler.scala:2206)\n\tat scala.collection.mutable.ResizableArray.foreach(ResizableArray.scala:62)\n\tat scala.collection.mutable.ResizableArray.foreach$(ResizableArray.scala:55)\n\tat scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:49)\n\tat org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:2206)\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1(DAGScheduler.scala:1079)\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1$adapted(DAGScheduler.scala:1079)\n\tat scala.Option.foreach(Option.scala:407)\n\tat org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:1079)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:2445)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2387)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2376)\n\tat org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:49)\n\tat org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:868)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2196)\n\tat org.apache.spark.sql.execution.datasources.FileFormatWriter$.write(FileFormatWriter.scala:200)\n\t... 32 more\nCaused by: java.io.IOException: Mkdirs failed to create file:/home/geno1664/Developments/Github_Samples/RDS-ENV/Rural_Development_Study_No2/IO/output.csv/_temporary/0/_temporary/attempt_202109091118556258811576982811410_0051_m_000033_2265 (exists=false, cwd=file:/home/hadoop/.workspace/app-20210909111836-0003/0)\n\tat org.apache.hadoop.fs.ChecksumFileSystem.create(ChecksumFileSystem.java:458)\n\tat org.apache.hadoop.fs.ChecksumFileSystem.create(ChecksumFileSystem.java:443)\n\tat org.apache.hadoop.fs.FileSystem.create(FileSystem.java:1118)\n\tat org.apache.hadoop.fs.FileSystem.create(FileSystem.java:1098)\n\tat org.apache.hadoop.fs.FileSystem.create(FileSystem.java:987)\n\tat org.apache.spark.sql.execution.datasources.CodecStreams$.createOutputStream(CodecStreams.scala:81)\n\tat org.apache.spark.sql.execution.datasources.CodecStreams$.createOutputStreamWriter(CodecStreams.scala:92)\n\tat org.apache.spark.sql.execution.datasources.csv.CsvOutputWriter.<init>(CsvOutputWriter.scala:38)\n\tat org.apache.spark.sql.execution.datasources.csv.CSVFileFormat$$anon$1.newInstance(CSVFileFormat.scala:84)\n\tat org.apache.spark.sql.execution.datasources.SingleDirectoryDataWriter.newOutputWriter(FileFormatDataWriter.scala:126)\n\tat org.apache.spark.sql.execution.datasources.SingleDirectoryDataWriter.<init>(FileFormatDataWriter.scala:111)\n\tat org.apache.spark.sql.execution.datasources.FileFormatWriter$.executeTask(FileFormatWriter.scala:269)\n\tat org.apache.spark.sql.execution.datasources.FileFormatWriter$.$anonfun$write$15(FileFormatWriter.scala:210)\n\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:90)\n\tat org.apache.spark.scheduler.Task.run(Task.scala:131)\n\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$3(Executor.scala:497)\n\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1439)\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:500)\n\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n\t... 1 more\n"
     ]
    }
   ],
   "metadata": {}
  }
 ],
 "metadata": {
  "orig_nbformat": 4,
  "language_info": {
   "name": "python",
   "version": "3.8.10",
   "mimetype": "text/x-python",
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "pygments_lexer": "ipython3",
   "nbconvert_exporter": "python",
   "file_extension": ".py"
  },
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3.8.10 64-bit ('RDS-ENV': venv)"
  },
  "interpreter": {
   "hash": "bcb7c92ec30a48671b2bbe6ad838283083cf4acf6e28fba1bfae084ec93c4ac6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}